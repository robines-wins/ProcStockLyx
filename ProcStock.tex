%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{luainputenc}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{slashed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\noun}[1]{\textsc{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxlist}[1]
{\begin{list}{}
{\settowidth{\labelwidth}{#1}
 \setlength{\leftmargin}{\labelwidth}
 \addtolength{\leftmargin}{\labelsep}
 \renewcommand{\makelabel}[1]{##1\hfil}}}
{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[pdftex,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2]{hyperref}

\makeatother

\usepackage{babel}
\begin{document}

\title{Introduction to Stochastic Processes\\
MATH-447 McGiil}


\author{Robin Solignac\\
MJ Lagarde}

\maketitle
\tableofcontents{}


\chapter{conditional probabilities, expectation}


\section{conditional probabilities and expectation, definitions}

Definition: let's two random variables $X$ and $Y$ have a join discrete
probability function $P(x,y)=P(X=x,Y=y)$ for $(x,y)\subset A$ (called
the support of the distribution) and $P(X=x,Y=y)=0\mbox{ for }(x,y)\notin A$.

Then the conditional probability of $Y$ given $X=x$, $P_{y/X=x}(y|x)$
is defined to be $\frac{P_{X,Y}(x,y)}{P_{X}(x)}$ for$P_{X}(x)\neq0$.
Here $P_{X}(x)$ is called the marginal. $P_{X}(x)=\sum_{\forall y}P_{X,Y}(x,y)$. 


\paragraph{Notes : }
\begin{enumerate}
\item If you fix x, then $P{}_{Y|X}(y|x)$ consider as function of Y, defines
a conditional probability distribution of Y given x.
\item There is no such thing as the random variable $Y|X=x$. It is simply
defined as $P(Y=y|X=x)$ is defined as above. When you see the statement
$Y|X=x$ don't interpret this to mean that that the r.v had the $p$
function $P_{y/X=x}(y|X=x)$. It means $P_{y|X=x}(y|x)=P(Y=y|X=x)$. 
\end{enumerate}

\paragraph{Definition 2: }

If the random variables $(X,Y)$ are jointly discrete, the conditional
expectation of $Y$ given $X=x$, written $E(Y|X=x)$ is defined as
follows : $E(Y|X=x)=\sum_{\forall y}y*P_{y/X=x}(y|x)$. 


\paragraph{Definition 3: }

If the random variables $(X,Y)$ are jointly continuous with probability
density function (pdf) $f_{X,Y}(x,y)$ then we define conditional
pdf of $Y$ given $X=x$ as follows : $f_{Y|X}=x(y|x)=\frac{fX,Y(x,y)}{fX(x)}$

Here $\intop_{-\infty}^{\infty}f_{X,Y}(x,y)\,dy$ is called marginal
pdf of $X$.


\paragraph{Definition: }

If the r.v. $X$ and $Y$ are continuous, with joint pdf $f_{X,Y}(x,y)$
then we define the conditional expectation of $Y$ given $X=x$ as
follows $E(Y|X=x)=\intop_{-\infty}^{\infty}y*f_{Y|X=x}(y|x)\,dx$


\section{The laws of total probability and total expectation}


\subsection{Law of total probability : }

Let X,Y be random variables with some join discrete distribution or
continuous distribution.
\begin{lyxlist}{00.00.0000}
\item [{(I)}] If discrete$P_{Y}(y)=\sum P_{Y|X=x}(y|x)*P_{X}(x)=\sum P(X=x,Y=y)$ 
\item [{(II)}] If continuous :$F_{y}(y)=\int f_{Y|X}(y|x)*f_{X}(x)\,dx$
\item [{(I')}] $F_{Y}(y)=P(Y\leq y)=\sum P_{y/X=x}(y\leq y|x)*P_{X}(x)=\sum F_{y/X=x}(y|x)*f_{X}(x)\,dx$
where $\sum F_{y/X=x}(y|x)=p_{Y/X=x}(zx)z\,:\,z\leq y$
\item [{(II')}] $F_{Y}(y)=\intop p(y\leq y|X=x)*f_{X}(x)\,dx$
\end{lyxlist}

\subsection{Law of total expectation and total variance}

\[
E(Y)=E_{X}\left[E_{Y|X}\left(Y|X\right)\right]
\]



\paragraph{Note:}

How do we think of $E\left(Y|X\right)$ ? 

We have not define this since the conditioning ``things'' is itself
a \emph{random variable} (we only define $E\left(Y|X=x\right)$).\\
In fact $E\left(Y|X\right)$ is itself a random variable. For each
value of $X$ we'll get a different value of $E\left(Y|X\right)$.\\
Although in more advanced probabilities $E\left(Y|X\right)$ can be
define under very general conditions. We can give a working way of
thinking of $E\left(Y|X\right)$ under slightly stranger conditions:
\begin{enumerate}
\item Define $E\left(Y|X=x\right)$ as some $g(x)$
\item Define $E\left(Y|X\right)$to be $g(X)$
\end{enumerate}
$g(X)$ is of course a random variable as $X$ is one.

We extend the Law of total expectation to a \emph{law of total variance}:
\\
if $X$ and $Y$ have some distribution then we can write 
\[
\mbox{Var}(Y)=E_{X}\left[\mbox{Var}\left(Y|X\right)\right]+\mbox{Var}_{X}\left[E\left(Y|X\right)\right]
\]



\subsection{Application}


\paragraph{Ex: }

Let $X_{1},X_{1}$be identically distributed random variable with
common mean $E(X)=\mu$ let $N$ be a non-negative integer valued
r.v, independent of $X_{1,}X_{2},\ldots$ . Then $E\left[\sum_{i=1}^{N}X_{i}\right]=E(X)E(N)=E(N)\mu$


\paragraph{Note:}

We cannot simply write $E\left[\sum_{i=1}^{N}X_{i}\right]=\sum_{i=1}^{N}E\left[X_{i}\right]$.
Since the upper limit of summation is random and nothing in probs
allow us to take the expected value inside when the sum is random 


\paragraph{Proof:}

The presence of $\geq2$ r.v: $N$ and the $X_{i}$ suggest that conditioning
one one or more of them may make thing simpler. idea: if we condition
on $N=n$ then we have a fixed upper limit sum condition and then
$E\left[\sum_{i=1}^{n}X_{i}\right]=\sum_{i=1}^{n}E\left[X_{i}\right]$.

We use law of total expectation. Let $S_{n}=\sum_{i=1}^{n}X_{i}$
we have $E\left[S_{n}\right]=E_{n}\left[E\left[S_{n}|N\right]\right]$
and we need $E\left[S_{n}|N\right]=\left.E\left[S_{n}|N=n\right]\right|_{n=N}$.
now 
\begin{eqnarray*}
E\left[S_{n}|N=n\right] & = & E\left[\sum_{i=1}^{n}X_{i}|N=n\right]\\
 & = & E\left[\sum_{i=1}^{n}X_{i}\right]\\
 & = & \sum_{i=1}^{n}E\left[X_{i}\right]\\
 & = & n\mu
\end{eqnarray*}


since $N$ is independent with all $X_{i}$.

So $E\left[S_{n}|N\right]=\left.n\mu\right|_{n=N}=n\mu$. Finally
$E\left[S_{N}\right]=E\left[N\mu\right]=\mu E\left[N\right]$


\paragraph*{Warning:}

when you condition, retain the conditioning ``event'' until you've
decide that it can be removed. In the above $g(n)=E\left[S_{N}|N=n\right]$
so $g(N)=E\left[S_{N}|N\right]$


\subsection{Wald's identity}

the following extension is important since it allows us to remove
the assumption that $N$ is independent of $X_{1,}X_{2},\ldots$ its
called \emph{Wald's identity.} It has various form, the simplest one:

Let $X_{1,}X_{2},\ldots$ be identically distributed with $E(X)=\mu$
let be $N$ a \emph{stopping rule, }that is the decision to stop summing
at $N=n$. It depend only on $X_{1,}X_{2},\ldots,X_{n}$and not on
any $X_{i},\,i>n$. Remarkably we still has 
\[
E\left[\sum_{i=1}^{N}X_{i}\right]=E(N)E(X)=E(N)\mu
\]


To clarify what is a stopping rule, 2 example, one where $N$ is one,
one where not


\subparagraph{Ex1}

Stop summing at $N=n$ if and only if $\sum_{i=1}^{n-1}X_{i}<10$
and $\sum_{i=1}^{n}X_{i}\geq10$. Stop summing at $n$ depend only
of $X_{1,}X_{2},\ldots,X_{n}$


\subparagraph{Ex2}

Example where $N$ won't be the stopping rule:

Let $X_{1,}X_{2},\ldots$ denote the of cracks fond in a successive
weeks in an aircraft component. $S_{n}=\sum_{i=1}^{n}X_{i}$ = cumulative
number of cracks up to the random week, N. Now suppose N is defined
as follows: $N=n$, if $0$ new cracks occurs in week $n+1$. With
this stopping rule, the decision to stop adding the number of cracks
at month n depends o,n what happens after week n. Thus n is not a
stopping rule. 


\section{Probability Generating Function : }

The following ``generating function'' plays a role similar to moment
generating function, but it's definition is restricted to non-negative
integer valued r.v.s, whose support is on \{0,1,2..\}.


\paragraph{Definition }

Let X be a discrete r.v. with support on \{0,1,2..\}. The probability
generating function (pgf) is defined as follows: 
\[
\Phi_{X}(z)=\sum_{x=0}^{\infty}z^{x}p_{x}(x)=\sum_{x=0}^{\infty}z^{x}P\left[X=x\right]
\]
 for $|z|\leq1$

Notes: 
\begin{enumerate}
\item $\Phi_{X}(z)$ is a power series in z which converges for |z|$\leq1$
\item $\Phi_{X}(1)=1$
\item Reason for the name : Given $\Phi_{X}(z)$ one can generate or recover
$p_{x}(x)$ for all $x$. That is there is a one to one correspondence
between a pgf and its distribution. \\
Proof: Since $\Phi_{X}(z)$ is a power series in z it is uniformly
convergent for $|z|<1$. Hence $\Phi'_{x}(z)$ exists for such z and
$\Phi'_{x}(z)=\frac{d}{dz}\sum_{0}^{\infty}p_{x}(x)=\sum_{0}^{\infty}xz^{x-1}p_{x}(x)=\sum_{1}^{\infty}xz^{x-1}$
$\Phi_{x}'(0)=p_{x}(1)$\\
In a similar fashion, by differentiating successively, setting z=0,
we get $\Phi_{x/2!}^{"}(0)=\frac{p_{x}(z)}{z!}$ to give $p_{x}(x)$=$\Phi_{X}^{x}(0)/x!$
or in a more familiar notation : $P_{X}(k)$= $\Phi_{X}^{k}(0)/k!,$
for k=1.. with $\Phi_{X}^{0}(0)=p_{X}(0)=\Phi_{X}(0)$
\item By setting $z=e^{t}$ we obtain the mgf $\sum_{x}e^{tx}p_{x}(x)$
\item $\Phi_{X}(z)=E[z^{X}]$
\item It then follows from 5 that if $X_{1,}X_{2},\ldots,X_{n}$ are independents
r.v.s, then $\Phi_{\sum_{i=1}^{n}}(z)=E[z^{\sum_{1}^{n}Xi}]=E[\prod_{1}^{n}z^{Xi}]=\prod_{i=1}^{n}E[z^{Xi}]$=$\prod_{i=1}^{n}\Phi_{xi}[z]$
further if the Xiw are identically distributed then $\Phi_{\sum_{i=1}^{n}}(z):[\Phi_{x}(z)]^{n}$
\end{enumerate}
CHECK MY COURSES FOR ASSIGNEMENT:


\chapter{Stochastic processes themselves}


\section{Definition }

A stochastic process indexed by an index set $T$ is a family of random
variables, denoted by $\left\{ X{}_{t},\,t\in T\right\} $ such for
each $t\in T$, $X{}_{t}$ is a random variable.


\paragraph{Notes}
\begin{enumerate}
\item For each $t\in T$, $X{}_{t}$ is a r.v. means $X{}_{t}=X{}_{t}(\omega)$
is a function of $\omega\in S,$ the sample maps of the possible outcomes
of $X{}_{t}$ . That is for each t, $X{}_{t}$ had a probability distribution
that is specified by $F_{X{}_{t}}(.)$its cdf. 
\item For each fixed $\omega,$ $X{}_{t}$ is a function of $t\in T.$(the
idkr set). We call this function of t (for fixed $\omega\epsilon S$)
a trajectory or sample path of stochastic process. We may depic the
situation as follows. 
\item How is a stochastic process specified? Recall that if X is a r.v.
it is complety determined or specified once you have specified c.d.f
$F_{X}$. In the case of stochastic process it turns out that it is
uniquely specified once you have specified $F_{Xt1},$$F_{Xt2},$...,
$F_{Xtn}$ for all the collections of the r.v.s $X_{t1},$$X_{t2},..,X_{tn}$for
all $t1,t2,..,tn\in T$, and all $n$. 
\end{enumerate}

\paragraph{Notes:}

we can view stochastic process as follow, since $\omega\in S$ is
unknown in advance, each projectors in unknown in advance of our experiment.
Therefore, we can regard a stochastic process as a random function.
We are sitting in a function space. Each point in this space is a
function. Question that we may ask one ``What is the probability
that this function will fall in some set in this function space?

Extensen theorem is that the probabilities of all the event in this
function space is uniquely determined by the probabilities speciafied
by the finite themselves c.d.f.S.

Examples stochastic process:
\begin{enumerate}
\item Trivially any single r.v., $X$ is a stochastic process - the family
r.v.
\item the sequence of random variable $X_{1},X_{2},X_{3}$...
\end{enumerate}
recall knowing $P\left[X_{t1}\leq x_{1},\ldots,X_{tn}\leq X_{n}\right]=F_{X_{t1},\ldots,X_{tn}}^{(x_{1},\ldots x_{n})}\,\forall ti\forall n\forall-\infty<x_{i}<\infty$
gives the unique probabilities $P\left[\left\{ X_{t}\in B,t\in T'\right\} \right]$
(notes: this is useless)


\paragraph{Note:}

The join finite dimensional distribution are uniquely determined in
the case of a sequence of i.i.d r.v.s by the marginal distribution
$F_{X}(x)$ of any member of the sequence. : $F_{X_{t1},\ldots,X_{tn}}^{(x_{1},\ldots x_{n})}=\prod_{i=1}^{n}F_{X}(x_{i})$

Such sequences are not really the subject of study in stochastic processes
because of the assumption of independence


\section{Some non trivial example of stochastic processes}
\begin{enumerate}
\item $\left\{ x_{t},\,t\in D\right\} $where $X_{t}$ si the price of e
certain stock during day $D$. before the stock market opens. the
price over the day are uncertain. the trajectory of $X_{t}$ will
typically fluctuate and the $X_{i}$'s will be dependant.
\item $\left\{ X_{t},\,i\in\left\{ 1,2,\ldots\right\} \right\} $ where
$X_{t}$ denote the number of bacteria on e petri dish at minute $t$
from the time they start divinding
\item $\left\{ X_{t},\,i\in\left\{ 1,2,\ldots\right\} \right\} $ where
$X_{t}$ is the height of a dam above level $L$ at time $t$ 
\item if $\left\{ X_{1},X_{2},\ldots\right\} $is a sequence of i.i.d r.v.s,
let $S_{n}=\sum_{i=1}^{n}X_{i}$ then $\left\{ S_{n},\,n=1,2,\ldots\right\} $
is a stochastic process and is called a \emph{random walk.}\\
Note: while $X_{i}$s are independent, the $S_{n}$ are highly dependent
\end{enumerate}

\section{Jargon}
\begin{enumerate}
\item if index set is discrete we refer to a \emph{discrete parameter} stochastic
process\\
If continuous ($T$ uncountable) we refer to a \emph{continuous parameter}
stochastic process
\item If the set of possible value of $X_{t}$ contable (discrete) we refer
to a \emph{discrete state space}\\
If uncountable we refer to a \emph{continuous state space}
\end{enumerate}

\chapter{Branching Processes}

We will begin by formally defining it, and then mention some application


\section{Definition}

Let $\left\{ X_{n},n=0,1,2,\ldots\right\} $ be a stochastic process
of the following properties
\begin{enumerate}
\item $X_{0}=1$
\item the probability that this individual ($X_{0})$ gives rise to $j$
individuals at the next generation is $p_{j}$, for $j=0,1,2,\ldots$\\
i.e $P\left[X_{1}=j\,|\,X_{0}=1\right]=p_{j}\,\forall j$
\item \emph{Each} individual in the 1\textsuperscript{st }generation gives
rises to $j$ new individuals in the 2\textsuperscript{nd}generation
with probability $p_{j}$, for $j=0,1,2,\ldots$. Independently of
all other individuals in this generation and of the generation number
($n$), and so on for subsequence generations
\end{enumerate}
Let $X_{n}$ be the total size of the n\textsuperscript{th}generation
(not the cumulative sum over generation). Then $\left\{ X_{n},n=0,1,2,\ldots\right\} $is
called a \emph{branching process} (or Galton-Watson branching process)


\paragraph{Example:}
\begin{itemize}
\item Physics: nuclear reaction
\item Biology: occurrence of mutant gene over generation, survival of family
name
\end{itemize}

\section{Question we shall address}
\begin{enumerate}
\item what are the probability of eventual extinction of the population
?
\item What are then expected population size of the n\textsuperscript{th}generation
? The variance ?
\end{enumerate}
To answers 1. we shall need to work quite hard and we end up with
an interesting, perhaps unexpected, result. 

Even though we can view $\left\{ X_{n},n=0,1,2,\ldots\right\} $ as
a Markov chain, we shall answers these questions without reference
to them by using conditionnal expectation and probability generating
function. 

The details are important !!


\section{Answers:}

Let $p_{0}(n)$ denode the probability that there are no individuals
in the k\textsuperscript{th}. We want to find 
\begin{eqnarray}
P\left[\bigcup_{k=1}^{\infty}\left\{ X_{k}=0\right\} \right] & = & P\left[\lim_{n\rightarrow\infty}\bigcup_{k=1}^{n}\left\{ X_{k}=0\right\} \right]\nonumber \\
 & = & \lim_{n\rightarrow\infty}P\left[\bigcup_{k=1}^{n}\left\{ X_{k}=0\right\} \right]\label{eq:lim}
\end{eqnarray}


Now notes that $X_{n}=0\Rightarrow X_{n+1}=0$ . i.e becomes (\ref{eq:lim})
\[
\lim_{n\rightarrow\infty}P\left[X_{n}=0\right]=\lim_{n\rightarrow\infty}p_{0}(n)
\]


Let $\Phi_{X_{n}}(z)$ be the p.g.f of $X_{n}$, for any p.g.f we
have $\Phi(0)=p_{0}\Rightarrow p_{0}(n)=\Phi_{X_{n}}(0)$.

Thus it seems that the discussion reduce to one of p.g.f.s. this is,
indeed, the case. Let $\Phi(z)$ be the p.g.f of $X_{1}$, which of
course is the p.g.f of the r.v that gives the number that arise from
any single of individuals in\emph{ any} generation


\paragraph{Lemma:}

\begin{eqnarray}
\Phi_{X_{n}}(z) & = & \Phi_{X_{n-1}}\left(\Phi(z)\right)\label{eq:11}\\
 & = & \Phi\left(\Phi_{X_{n-1}}(z)\right)\;\forall n\geq1\label{eq:12}\\
\nonumber 
\end{eqnarray}


The ideae in proving both (\ref{eq:11}) and (\ref{eq:12}) is that
in each case we can write $X_{n}$ as a random sum of random variable.
with this observation we shall condition the upper limit of summation.


\subsection{Proof of the lemma }


\subsubsection{Proof of (\ref{eq:11})}

Observe that if we let $Y_{i}$ be the number of ``offspring'' from
parent $i$ in the (n-1)\textsuperscript{th}generation, then $X_{n}=\sum_{i=1}^{X_{n-1}}Y_{i}$


\subparagraph{Note:}

that we haven't not attached a subscript $n-1$ to $Y_{i}$ (i.e $Y_{i,n-1}$)
since the distribution of $Y_{i,n-1}$ is the same for all $n$ by
the assumption of branching processes. 

We now need $\Phi_{X_{n}}(z)=\Phi_{\sum_{i=1}^{X_{n-1}}Y_{i}}(z)$.
Looks bad, but observe that if we condition on $X_{n-1}=k$, than
we would be dealing with $\Phi_{\sum_{i=1}^{X_{n-1}}Y_{i}}(z)$ the
p.g.f of a fixed sum of i.i.d r.v.s. The $Y_{i}$'s re independent
and identically distributed by assumption.

Thus use $\Phi_{X_{n}}(z)=E\left[z^{X_{n}}\right]=E_{X_{n-1}}\left[E\left[z^{\sum_{i=1}^{X_{n-1}}Y_{i}}|X_{n-1}\right]\right]$

now: 
\begin{eqnarray*}
E\left[z^{\sum_{i=1}^{X_{n-1}}Y_{i}}|X_{n-1}=k\right] & = & E\left[z^{\sum_{i=1}^{k}Y_{i}}|X_{n-1}=k\right]\\
 & = & E\left[z^{\sum_{i=1}^{k}Y_{i}}\right]\mbox{indep of \ensuremath{\left\{ X_{n-1}=k\right\} }and \ensuremath{\left\{ Y_{1},\ldots,Y_{k}\right\} }}\\
 & = & \Phi_{\sum_{i=1}^{k}Y_{i}}(z)\\
 & = & \left[\Phi_{X_{1}}(z)\right]^{k}=\left[\Phi(z)\right]^{k}
\end{eqnarray*}
Note that's because$Y_{i}$ as the same distribution as $X_{i}$.

Finally $E_{X_{n-1}}\left[\left(\Phi(z)\right)^{X_{n-1}}\right]=\Phi_{X_{n-1}}\left(\Phi(z)\right)$
(by the definition of $\Phi_{X_{n-1}}$).

this proves (\ref{eq:11})


\subsubsection{Proof of (\ref{eq:12})}

to get (\ref{eq:12}) we we argue slightly differently. The no in
the n\textsuperscript{th}generation is the sum The sum of the number
that come from the sub generation of each ``parent'' in the first
generation.

The number that end up $(n-1)$ generation after each parent in the
1\textsuperscript{st}are i.i.d r.v.s, Call them $X_{n-1,i}\,i=1,\ldots,X_{1}$.
Thus $X_{n}=\sum_{i=1}^{X_{1}}X_{n-1,i}$. 

Using the same type of argument, as for (\ref{eq:11}) (Conditioning
on $X_{1}$ this time), we get 
\begin{eqnarray*}
E\left[z^{X_{n}}\right] & = & E_{X_{1}}\left[E\left[z^{\sum_{i=1}^{X_{1}}X_{n-1,i}}|X_{i}\right]\right]\\
 & = & \Phi_{X_{1}}\left[\Phi_{X_{n-1}}(z)\right]=\Phi\left[\Phi_{X_{n-1}}(z)\right]
\end{eqnarray*}


These equality have been established for all $n\geq1$ and are easily
seen (do this) to be true for $n=1$


\subsection{Answer of 1.}

(\ref{eq:11}) and (\ref{eq:12}) form the basis for the completion
of the analysis :

\[
\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)=\lim_{n\rightarrow\infty}\Phi_{X_{n-1}}\left(\Phi(0)\right)=\lim_{n\rightarrow\infty}\Phi\left(\Phi_{X_{n-1}}(0)\right)
\]


We still examine $\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)$, but
first we need to justify the existence of this limit.


\paragraph{Note:}

that $\Phi_{X_{n}}(z)$ is bounded for all $|z|\leq1$ and for all
$n$ $\Phi_{n}(z)\leq\sum_{k=0}^{\infty}|z|^{k}P\left[X_{n}=k\right]\leq\sum_{k}P\left[X_{n}=k\right]=1$.

Since $\Phi_{X_{n}}(0)=P\left[X_{n}=0\right]$ and since $\left\{ X_{n}=0\right\} \subset\left\{ X_{n+1}=0\right\} $
then$P\left[X_{n}=0\right]\leq P\left[X_{n+1}=0\right]$

And so$\left\{ \Phi_{X_{n}}(0)\right\} $is a bounded non decreasing
sequence and thus $\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)$ exist
$\eta$.


\paragraph{Recall:}

$\Phi_{X_{n}}(z)=\Phi_{X_{n-1}}\left(\Phi(z)\right)=\Phi\left(\Phi_{X_{n-1}}(z)\right)\;\forall n\geq1$.
(\ref{eq:11}) then (\ref{eq:12})

And so using using (\ref{eq:12}) $\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)=\lim_{n\rightarrow\infty}\Phi\left(\Phi_{X_{n-1}}(0)\right)=\Phi\left(\lim_{n\rightarrow\infty}\Phi_{X_{n-1}}(0)\right)$
($\Phi$ is continuous). this implies $\eta=\Phi(\eta)$ so the limit
must satisfy this properties.

We need to examine the root of $z_{0}=\Phi(z_{0})$.

It's easy to see that $\Phi(z)$ is convex in $z$. so differentiate
twice.

For now on, assume that $P\left[X_{1}=0\right]>0$, if not the probability
of extinction will be $0$. We have the following pictures (non)

We see that $z=1$ is a root since $\Phi_{X_{n}}(1)=1$. We see that
there's 2 possible roots.

We shall see that $\eta$ is the smallest possible root. 

We have $p_{0}(1)=\Phi(0)\leq\Phi(z_{0})=z_{0}$ for any possible
root $z_{0}$ of $\Phi(z)=z$. Using (\ref{eq:11})$p_{0}(z)=\Phi_{X_{2-1=1}}\left(\Phi(0)\right)=\Phi(z_{0})=z_{0}$.
and so on for all $p_{0}(n)$ 

So: $\lim_{n\rightarrow\infty}p_{0}(n)=\eta\leq z_{0}$. And so $\eta$
is the smallest positive root of $\Phi(z)=z$.


\paragraph{Question:}

Under what circumstance is there only one positive root ? that is,
when will the only root be $\eta=1$.

From the pictures we see that there's exactly one root (at $z=1$)
if and only if $\Phi'(1)\leq1$. But $\Phi'(1)=\mu=E\left(X_{1}\right)$


\paragraph{So:}

the branching process will end in extinction if $\mu\leq1$ we will
have a probability of extinction $\eta>0$ if $\mu>1$ (i.e there's
is pos probability of non-extinction). note that is never the case
that $\eta=0$ : $\mu>1\Rightarrow\eta>0\wedge\eta<1$


\paragraph{Notes:}
\begin{enumerate}
\item In practice if a population is to become extinct, this happens quickly
\item if we start with $a$ individuals (instead of $1$) then the probability
of extinction of \emph{all} descendant is $\eta^{a}$ (since the lines
develops independently)\\
thus, with $a$ to star with, the probability of non extinction is
$1-\eta^{a}$


This has interesting application in nuclear fission (in nuclear chain
reaction) because obviously we want the reaction (branching process)
to die out


Even if $\eta$ is large, $\eta^{a}$ will be small if $a$ (the ``critical
mass'') is taken to be \emph{very} large, in this case $\eta^{a}$
will be small and the probability that the nuclear reaction will be
maintained $1-\eta^{a}$ will be large

\item one extension to the Glaton branching-process is the so called Branching
process with Random Environment. the ``divison probability'' $p_{j}$
change over time. For example, the environment might changing (is
it !) leading to different reproduction rates
\item Another generalization is to Multitype Branching Process. Whereby
a population is composed of different subtype each with a different
$\left\{ p_{j}\right\} $
\end{enumerate}

\subsection{What is $E\left(X_{n}\right)$ and $\mbox{Var}\left(X_{n}\right)$
? (Answer of 2.)}

Let $\mu_{n}=E\left(X_{n}\right)$. Then $\mu_{n}=\Phi'_{X_{n}}(1)$.
But we know that $\Phi_{X_{n}}(z)=\Phi_{X_{n-1}}\left(\Phi(z)\right)$(\ref{eq:12})

Then we have $\mu_{n}=\Phi'_{X_{n}}(z)=\Phi'(z)\Phi_{X_{n-1}}'(z)$
(chain rule)

Which mean that $\Phi'_{X_{n}}(z)=\mu_{n}=\mu\Phi'_{X_{n-1}}(1)=\mu\mu_{n-1}$
which gives immediately $\mu_{n}=\mu^{n}$

Thus is $\mu>1$ the mean population grows geometrically , and if
$\mu<1$ it decline very quickly (geometrically)), If $\mu=1$ then
the population size is constant ($=1$).

To obtain $\mbox{Var}\left(X_{n}\right)$ we use (\ref{eq:12}) once
more (see 2) bur argument is more difficult. We find: 

\begin{eqnarray*}
\mbox{Var}\left(X_{n}\right)=\sigma_{n}^{2} & = & \sigma^{2}\mu^{n-1}\frac{\left[\mu^{n}-1\right]}{\mu-1},\mbox{ if \ensuremath{\mu\neq1}}\\
 & = & n\sigma^{2}\mbox{, if \ensuremath{\mu=1}}
\end{eqnarray*}
Thus $\sigma_{n}^{2}$increase (decrease) geometrically if $\mu\neq1$,
possibly explaining why $\eta>0$ even if $\mu>1$: large fluctuation
gice a chance of falling into the breach of extinction.


\chapter{Markov Chains}

Almost always there is an underlying dependence structure to the stochastic
processes that we discuss. A special type of dependence structure
is assumed when we discuss markov chain, this structures assumes the
so calls \emph{Markov property} 

To begin, we shall assume that our process$\left\{ X_{n},\,n=0,1,\ldots\right\} $
is a discrete parameter countable state space process. The state of
space of $\left\{ X_{n},\,n=0,1,\ldots\right\} $ could be denoted
$E_{1,}$$E_{2}..$ but for notation simplicity, by, $1,2,\ldots$


\section{Definition of a Markov chain}


\paragraph{Definition:}

$\left\{ X_{n},\,n=0,1,\ldots\right\} $ is called a (discrete state
space) Markov chain if

$P\left[X_{n+1}=j|X_{0}=i_{0},X_{1}=i_{1},...,X_{n}=i\right]=P\left[X_{n+1}=j|X_{n}=i\right]=P_{ij}$for
all $n\geq0$, and all states $i_{0},i_{1},\ldots,i_{n}$


\paragraph{Notes:}

1) $P_{ij}$is called the one step transition probability of the chain.
It describes the probability of moving from at state $i$ to a state
$j$ in one state. 

Notice that $P_{ij}$ is assumed not to depend on n. 

i.e. we should really write $P^{n}{}_{ij}$. Chain for which $P_{ij}$
does not depend on $n$ are called \emph{homogeneous}. {[}If depend
on n, non-homogeneous{]}. We restrict ourselves to homogenous chains. 

2) Importantly, the Markov property says that the history is summarized
in the immediate past. 

It does not assert that $X_{n+1}$is independent of $X_{0},X_{1},\ldots,X_{n-1}$

Example: Our branching process is easily seen to be a Markov chain,
since given the generation size, $X_{0},X_{1},..,X_{n-1},X_{n}$ the
distribution of $X_{n+1}$depends only on $X_{n}$

4)The term homogeneous is sometimes called stationary i.e. we talk
of a stationary Markov Chain. 

Reason(to be further clarified later) is that with the assumption
of homogeneity (stationarity) the join finite dimensional distributions,
are invariate under shifts in time.


\subparagraph{Exemple of non-stationary Markov chain:}

A branching process with the number of childen from each from each
parent in a generation, having a distribution that deoends on the
generation number in fertility changes with n. 

5) a process that satisfies the Markov property above is called 1st
order Markov. If the conditional probability reduced to what the previous
2 states were, we called such a process 2nd order Markov.. etc. 


\section{How does all of this work? }

We wish to model a sequence of dependent r.v.s. To do this requires
specifying the joint finite dimensional distribution - a hard problem.
However, if we can assume the special dependence structure induced
by the Markov property then we'l see that the specification of the
finite dimensional distribution is make much simpler. Of course, one
has to be satisfied that the Markov property holds. There are statistical
tests that can be used to test the Markov asssumption. 

Terminology:

The one step transmission probabilities are denoted by$P_{ij}$. The
complete set of $P_{ij}$ is displayed in the transition matrix:

$P=\begin{bmatrix}p_{11} & p_{12} & \cdots\\
p_{21} & p_{22} & \cdots\\
\cdots & \cdots & \ddots
\end{bmatrix}$

Some $P_{ij}$ may be 0. We have immediately that $\sum_{j}P_{ij}=1$
for all $i$. You have to go some-where from state i. $P$ is called
a stochastic matrix - non negative entries sum to one. Of course the
matrix could be finite corresponding to $m$ states. 

Precision: $P_{ij}^{(n)}=P\left[X_{n}=j|X_{0}=i\right]$ = Probs of
going from $i$ to $j$ in $n$ steps

The aim here is to find an expresion for $p_{ij}^{(n)}$, or at least
a mean of obtaining $p_{ij}^{(n)}$. In principle, the way is through
what we call the \noun{Chapman-Kolmogorov} equations


\subsection{Theorem:}

let $P^{(n)}=\left\{ p_{ij}^{(n)}\right\} $ be the matrix whose entries
are the n-step transition probabilities $p_{ij}^{(n)}$. Then 
\[
P^{(m+n)}=P^{(m)}P^{(n)}
\]


a corollary will be 
\[
P^{(n)}=P^{n}=\left[\left\{ p_{ij}\right\} \right]^{n}
\]



\subsection{Proof}


\paragraph{Theorem proof:}

\begin{eqnarray*}
P\left[X_{m+n}=j|X_{0}=i\right] & = & \sum_{k}P\left[X_{m+n},X_{m}=k|X_{0}=I\right]\\
 & = & \sum_{k}\frac{P\left[X_{m+n}=j,\,X_{m}=k,\,X_{0}=i\right]}{P\left[X_{0}=i\right]}\\
 & = & \sum_{k}\frac{P\left[X_{m+n}=j|X_{m}=k,\,X_{0}=i\right]P\left[X_{m}=k|X_{0}=i\right]P\left[X_{0}=i\right]}{P\left[X_{0}=i\right]}\\
 & = & \sum_{k}\underbrace{P\left[X_{m+n}=j|X_{m}=k\right]}_{\mbox{Markov prop}}P\left[X_{m}=k|X_{0}=i\right]\\
 & = & \sum_{k}\underbrace{p_{ik}^{(m)}p_{kj}^{(n)}}_{\mbox{Homogenity }}=p_{ij}^{(m+n)}
\end{eqnarray*}


Which shows that $p_{ij}^{(m+n)}$ is just the i-j\textsuperscript{th}elements
of the product of the matrices $P^{(m)}$ and $p^{(n)}$, i.e: $P^{(m+n)}=P^{(m)}P^{(n)}$


\paragraph{Corollary proof:}

In particular take $m=1,\,n=1,\,\left[n+m=2\right]$ then $P^{(2)}=P.P=P^{2}$
and then $P^{(3)}=P.P^{(2)}=P.P^{2}$ , ect\ldots{} This can be generalize
by induction to $P^{(n)}=P^{n}$

In principle if the chain finite (it has finite many step) we can
find $P_{ij}^{(n)}$ by doing succesive matrix multiplication.

If the number of step is infinite then this does not work and we will
se that w often end up having to approximate $P_{ij}^{(n)}$ for large
$n$ by $\lim_{n\rightarrow\infty}P_{ij}^{(n)}$ (Still to come)


\subsection{Some example.}


\subsubsection{Exemple 1}

Consider a communication system wich transmit the digits 0 or 1. Each
digit transmitted must pass through serval stagr at each of which
there's a prob $p$ that the digit will be unchanged when it leaves
the stage, let $X_{n}$ the digits the $n^{th}$ stage then $\left\{ X_{n},n=0,1,\ldots\right\} $
is a 2-state markov chain with $P=\left[\begin{array}{cc}
p & 1-p\\
1-p & p
\end{array}\right]$


\subsubsection{Exemple 2 The simple random Walk}

This is a basic well-studied non-trivial example of a markov chain
that is constructed from basic properties easy to imagine.

Suppose that a particle can jump between the integers ($\mathbb{Z}$)
on the x-axis acording to the following rules
\begin{enumerate}
\item It can move either form it current position (integer) by moving either
1 step to the left, or 1 step to the right
\item it moves independently of how many chain it has taken
\item it move 1 step to right with probability $p$ no matter what is its
current position, and 1 step to the left with prob $1-p$
\item it moves independently of its previous move 
\end{enumerate}
Let $X_{n}$ be the position of the particle after $n$ moves. we
call $\left\{ X_{n},\,n=0,1,2,\ldots\right\} $ the \emph{simple random
walk} 


\paragraph{Claim:}

$\left\{ X_{n},\,n=0,1,2,\ldots\right\} $ form a markov chain.

We begin by writing down the transition matrix 
\[
P=\left[\begin{array}{ccccccc}
\ddots & \cdots & \cdots & \cdots & \cdots & \cdots & \vdots\\
\vdots & 0 & p & 0 & 0 & 0 & \vdots\\
\vdots & 1-p & 0 & p & 0 & 0 & \vdots\\
\vdots & 0 & 1-p & 0 & p & 0 & \vdots\\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \ddots
\end{array}\right]
\]
(This matrix is centered on 0).

Next to show that $\left\{ X_{n},\,n=0,1,2,\ldots\right\} $ is a
markov chain, write $X_{n}$ as $X_{n}=\sum_{l=0}^{n}y_{l}$ where
$y_{l}$ are r.v.s such that $P\left[y_{l}=+1\right]=p,\,P\left[y_{l}=-1\right]=1-p$. 

These $y_{l}$ are the step sizes, $X_{n}$ is a result of summing
the steps to the right plus to the left. Now let's check the Markov
properties

\begin{eqnarray*}
P\left[X_{n+1}=j\,|\,X_{0}=0,X_{1}i_{1},\ldots,X_{n}=i\right] & = & P\left[\sum_{l=0}^{n+1}y_{l}=j\,|\,X_{0}=0,\sum_{l=0}^{1}y_{l}=i_{i},\ldots,\sum_{l=0}^{n}y_{l}=i\right]\\
 & = & P\left[\sum_{l=0}^{n}y_{l}+y_{n+1}=j\,|\,X_{0}=0,\ldots,\sum_{l=0}^{n}y_{l}=i\right]\\
 & = & P\left[\left(i+y_{n+1}\right)=j\,|\,\ldots\right]\\
 & = & P\left[y_{n+1}=j-i\,|\,X_{0}=0,\ldots,\sum_{l=0}^{n}y_{l}=i\right]\\
 & = & P\left[y_{n+1}=j-i\right]
\end{eqnarray*}


Because in fact because $y_{n+1}$ is independent of all the sums
to the n\textsuperscript{th} by assumtion (3). But $P\left[y_{n+1}=j-i\right]=P\left[\sum_{l=0}^{n+1}y_{l}=j\,|\,\sum_{l=0}^{n}y_{l}=i\right]$
which establishes the markov property.

Further, $P\left[y_{n+1}=j-i\right]=\begin{cases}
p & j=i+1\\
1-p & j=i-1\\
0 & \left|j-i\right|>1
\end{cases}$


\subsubsection{Exemple 3}

Consider a gambler who at each play wins \$1 with prob $p$ and loses
\$1 with prob $1-p$. Supose that the gambler stop playing whe she
goes broke or atteins \$$N$.

Let $X_{n}$ be the gambler fortune after $n$ games. We claim that
$X_{n},\left\{ n=0,1,\ldots\right\} $ is a Markov chain with what
one known as absorbing stads at $0$ and $N$: once you are at $0$
or $N$ you remain there with prop $1$ for all $i$. You have to
go some-where from state $i$.


\section{Communcating, and classification of states}


\subsection{Finite dimensional distributions}

$P$ is called a stochastic matrix - non negative entries sum to one.
Of course the matrix could be finite corresponding to $m$ states.
The following theorem says that the knowing $P[X_{0}=i]$ for $i=1,2,...${[}the
so called initial distribution{]} and the transition matrix $P$ is
sufficient to completely specify the markov chain.

Let $\left\{ X_{n},n=0,1..\right\} $ be a Markov chain. Then, its
finite dimensional distributions are uniquely determined by $\left\{ P[X_{0}=i],i=1,2,..,P\right\} $


\subsubsection*{Proof:}

We must know that we can wite the joint distribution of any set $X_{n1},X_{n2}....$
in term of $P[X_{0}=i]$ and a ``bunch'' of i-step transition probabilities
$P_{ij}$

We convey the idea through a special case: Thus consider $P[X_{2}=k,X_{3}=l]$

Thus 
\begin{eqnarray*}
P[X_{2}=k,X_{3}=l] & = & P[X_{3}=l|X_{2}=k]*P[X_{2}=k]\\
 & = & P\ensuremath{_{kl}*\sum_{i,j}P[X_{2}=k\cap X_{0}=i,}\ensuremath{X_{1}=j]}\\
 & = & P_{kl}*\sum P[X_{2}=k|X_{1}=j]*P[X_{i}=j|X_{0}=i]*P[X_{0}=i]\\
 & = & P_{kl}\sum_{i,j}P_{ij}P_{jk}P[X_{0}=i]
\end{eqnarray*}
which is a function only of the i-step transition probs and $P[X_{0}=i]$.

The idea is simply that to find the probabilities of going from state
$i$ to state $j$ in $n$ steps you sum the probabilities of all
paths that take you from $i$ to $j$ in $n$ steps. Next the prob
of each path is the prob of an intersection of the form 
\begin{eqnarray*}
P\left[A_{1}\cap A_{2}...A_{n}\right] & = & P\left[A_{n}|A_{1}\cap A_{2}\ldots A_{n-1}\right]*P\left[A_{1}\cap A_{2}\ldots A_{n-1}\right]\\
 & = & P\left[A_{n}|A_{n-1}\right]*P\left[A_{n-1}|A_{n-2}\right]*\ldots*P\left[A_{1}\right]
\end{eqnarray*}


Let$P_{ij}^{n+m}$be $P[X_{m+n}=j|X_{0}=i]$

It is easy to see that the markov propety is satisfied (check this
formality).

In this case we have : 

-- 0- 1- 2- .... N 

0 1 - 0- 0- 0- 0 

1 1-p - 0 - p ...

2 0- 1-p - 0- p 


\subsection{The classification of states}

Eventually we'll be studing the limiting behavior of $p_{ij}^{(n)}$as
$n\rightarrow\infty$. In order to do so require the classificatinon
of the states of a Markov chain and the classification of the chain
themselves


\subsubsection*{Definition: }

Let $\left\{ X_{n},n=0,1,2,..\right\} $ be a Markov chain with transmission
matrix $P=\{P_{ij}\}.$Then we say that the state $j$ is accessible
from the state $i$, if and only if there exist an $n>0$ s.t. $p_{ij}^{(n)}>0$.
{[}i.e. there is a positive prob of reaching state j from state i
in a finite number of steps{]}

Note that $p_{ij}^{(n)}$is not the probability of reaching j from
i for the first time in n steps. 

We write $i\rightarrow j$ to mean ``j is accessible from i''


\paragraph{Definition: }

If $i\rightarrow j$ and $j\rightarrow i$ then we say that $i$ and
$j$ \emph{communicate}, and write $i\leftrightarrow j$


\paragraph{Theorem:}

If $i\leftrightarrow j$ and $j\leftrightarrow k$, then $i\rightarrow k$
and conversly


\subparagraph{Corrolary: $i\leftrightarrow j\wedge j\leftrightarrow k\Rightarrow i\leftrightarrow k$}

Proof: $i\rightarrow j\Rightarrow\exists n_{0}>0$ such that $p_{ij}^{(n)}>0$
and $j\rightarrow k\Rightarrow\exists n_{1}$ such that $p_{jk}^{(n)}>0$

Let $n=n_{0}+n_{1}$

Then $p_{ik}^{(n)}=p_{ik}^{(n_{0}+n_{1})}=\sum_{l}p_{il}^{(n_{0})}*p_{lk}^{(n_{1})}\geq p_{ij}^{(n_{0})}*p_{jk}^{(n_{1})}>0$

$i\rightarrow k$ done!

The corollary is immediate same $i\leftrightarrow j\Leftrightarrow j\leftrightarrow j$


\subsubsection{Definition of states}

Given a state j of a Markov chain, its communicating class $C{}_{j}$
is defined to be the set of all state that communicate with $j$,
in the sense that $p_{jk}^{(n)}>0$ for some $n_{0}>0$ $P_{kj}^{(n)}>0$
for some $n_{1}>0.$ It may happen that $C_{j}$ is empty {[}i.e.
j communicates with no state, not even with itself{]}. In this case
we say that $\left\{ j\right\} $ is a non-return state. 

If $C_{j}$ is non-empty, then, $j\in C_{oj}$, to see this note that
$C_{j}$ non empty implies that there must exist a.k such that $j\rightarrow k$
(and $k\rightarrow j$)

$\exists n_{0}\,:\,p_{jk}^{(n0)}>0$ and $\exists n_{1}\,:\,p_{kj}^{(n1)}>0$.
Let $n=n_{0}+n_{1}$. Then $P_{jj}^{(n)}\geq P_{jk}^{(n0)}P_{kj}^{(n1)}>0$.
$j$ communicates with itself, and this $j\in C_{j}$

A state which communicates with itself is called \emph{return state}.
A non-empty class, $C$ of state in a Markov chain is to be a \emph{communicating
class} if for some state $j$ s.t $C_{k}=C$ . That is $C$ is the
class of all states that communicate with one-another


\subsubsection*{Theorem: }

If $C_{1}$and $C_{2}$are communicating classes, then either $C_{1}=C_{2}$or
$C_{1}\cap C_{2}=\slashed{O}$


\subsubsection*{Proof: }

Let $C_{1}\cap C_{2}\neq\slashed{O}$ such that $C_{1}\neq\slashed{O}$
and $C_{2}\neq\slashed{O}$ so $\exists i\in C_{2}\,\exists j\in C_{1}\cap C_{2}$
but $C_{1}\cap C_{2}\subset C_{1}$so $i\leftrightarrow j$

\begin{eqnarray*}
\exists i\in C_{1}\\
\exists j\in C_{1}\cap C_{2}\\
\exists k\in C_{2}\\
\mbox{but}\\
C_{1}\cap C_{2}\subset C_{1}\\
C_{1}\cap C_{2}\subset C_{2}\\
\therefore i\rightarrow j\\
\therefore j\rightarrow k\\
\therefore i\rightarrow k & \mbox{transitivity of "accessibility"}\\
\therefore C_{1}\subset C_{2}\wedge C_{2}\subset C_{1}\\
\therefore C_{1}=C_{2}
\end{eqnarray*}



\subsection{Decomposition of a Markov chain into disjoint classes}


\subsubsection*{Theorem: }

The set $S$ of states of a Markov chain can be written as the union
of a finite or countably infinite family $\left\{ C_{r}\right\} $of
disjoint sets of states. $S=C_{1}\cup C_{2}...\cup C_{r}$ with $C_{i}\cap C_{2}=\slashed{O}$

Each class $C_{r}$ is either a communicating class or contains exactly
one non return state 


\subsubsection*{Proof }

Choose a state $j_{1}$ and let $C_{1}=C_{j_{1}}$, the class of all
the states that communicate with $j_{1}$according as $j_{1}$is a
return or non-return state, if $j_{1}$is a non-return state, then
set $C_{1}=\left\{ j_{1}\right\} $ 

Then choose $j_{2}$ not in $C_{1},$and let $C_{2}=C_{j_{2}}$or
$\left\{ j_{2}\right\} $ depending on whether or not \{$j_{2}\}$
is a non return state. From the previous theorem, $C_{1}\cap C_{2}=\slashed{O}$
Continue the construction in this way to get $C_{1},C_{2},\ldots$

A non empty class, $C$, of state is such that to be closed if no
state outside $C$ is accessible from any state of $C$. 

i.e. $C$ is closed iff for every for every $\forall n>0\,\forall j\in C\,\forall k\notin C\;P_{jk}^{(n)}=0$, 

Examples: In the following examples we classify the states of the
corresponding Markov chains with the given transition matrices.
\begin{enumerate}
\item $\begin{bmatrix}1 & 0 & 0 & 0\\
\frac{1}{2} & \frac{1}{2} & 0 & 0\\
\frac{1}{3} & \frac{1}{3} & 0 & \frac{1}{3}\\
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4}
\end{bmatrix}$

\begin{itemize}
\item \{1\} closed communicating
\item \{2\} communication non closed
\item \{3-4\} communication non closed
\end{itemize}
\item $\begin{bmatrix}1 & 0 & 0 & 0\\
1 & 0 & 0 & 0\\
\frac{1}{2} & \frac{1}{2} & 0 & 0\\
\frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0
\end{bmatrix}$

\begin{itemize}
\item \{4\} non return 
\item \{1\} closed communicating
\item \{2\} non return (non closed)
\item \{3\} non return
\end{itemize}
\end{enumerate}
A \emph{closed communication} class $C$ of states is essentially
a Markov chains extracted and studied independently

If you write the transition matrix so that the states in $C$ are
written, then $P$ can be written in the form $P=\begin{bmatrix}P_{C} & 0\\
* & *
\end{bmatrix}$ where $*$ are possibly non-zero matrix and $P_{c}$ is the sub-matrix
whose entries are the transition probabilities between the states
of $C$. Then $P^{(n)}=P^{n}=\begin{bmatrix}P_{C}^{n} & 0\\
* & *
\end{bmatrix}$

Since a closed communication class $C$ can be extracted from a Markov
chain (by deleting all rows and colons corresponding to states outside
of $C$) and treated by itself as a Markov chains it follows that
for the study asymptotic properties of a Markov chain, it is sufficient
to restrict one self to chains in which there is exactly one closed
communicating classare all the other communicating class are non closed


\subsection{Futher Cassification of states.}

In order to obtain certain asymptotic properties of a Markov chain,
we shall make futher classification of states


\paragraph{Definition:}

For any state $j,k$ let $f_{jk}$ denote the probability starting
from $j$ the process will get into state $k$

Let $g_{kk}$ denote the probability that the chain will retrun infinitly
often to state $k$ given that it start in $k$


\paragraph{note:}

these definitions can be made mathematically rigorous by introducing
the r.v $N_{k}(n)$, the number of time that the state $k$ is visited
in $n$ step. Our descriptive definition will suffice


\paragraph{Definition:}

We denote $f_{jk}^{(n)}$ the conditionnal probability of entering
$k$ from $j$ from the first time in exactly $n$ steps 

the time it takes to enter $k$ from $j$ for the first time is called
the first passage time.


\paragraph{Note:}

$f_{jk}^{(n)}can$be made precise by formaly saying that if we are
to reach $k$ for the first time from $j$ in $n$ steps then the
chains must not have visited $k$ in the first $n-1$steps and then
on n\textsuperscript{th}stap have jumped to state $k$


\paragraph{Example:}

Let $\left\{ X_{n},n=0,1,\ldots\right\} $ be a 2-states Markov chain
(with state 0 and 1) find the 1\textsuperscript{th}passage probs
$f_{jk}^{(n)}.$ Ex: $f_{00}^{(n)}=P_{01}P_{11}^{n-2}P_{10}\,n\geq2$


\paragraph{Proof:}

it's a mistake ti say that the product (and power $n-2$) arise because
of independence of various event. They are certainly independent.
However we can condition backward and use the markov property.

\begin{eqnarray*}
f_{00}^{(n)} & = & P\left[X_{1}=1,X_{2}=1,\ldots,X_{n-1}=1,X_{n}=0|X_{n}=0\right]\\
 & = & P\left[X_{n}=0|X_{0}=0,X_{1}=1,\ldots,X_{n-1}=1\right]P\left[X_{n-1}=1|X_{0}=0,\ldots;,X=1\right]\\
 &  & \cdots P\left[X_{1}=1|X_{0}=0\right]\\
 & = & P\left[X_{n}=0|X_{n-1}=1\right]P\left[X_{n-1}=1|X_{n-2}=1\right]\cdots P\left[X_{1}=1|X_{0}=0\right]\\
 &  & \mbox{[We used Markov property]}\\
 & = & p_{01}p_{11}^{n-2}p_{10}\\
 &  & \mbox{[We used Homogeneity}]
\end{eqnarray*}


The other $f_{jk}^{(n)}$'s fllow similarly


\subsubsection*{Theorem:}

For any states $j$ and $k$ $f_{jk}=\sum_{n=1}^{\infty}f_{ik}$


\paragraph{Proof:}

Ever entering $k$ from $j$ is the disjoin union of entering $k$
from $j$ in exactly $n$ steps $\forall n\geq1$.


\subsubsection*{Theorem:}

For any states $j$ and $k$ and $n\geq1$ $P_{jk}^{(n)}=\sum_{m=1}^{n}f_{jk}^{(m)}P_{kk}^{m-n}$


\paragraph{Proof: }

If you are in state $k$ (starting in $j$) \emph{at} the n\textsuperscript{th}step
then you must have entered $k$ for the 1\textsuperscript{th}at some
$m$ and then in the remaining $n-m$ step, be in state $k$ (starting
from $k$)

we're talking of a disjoin union of these events. again we use the
Markov property and homogeneity to get the product


\subsubsection{Theorem: }

For any state $j$ and $k$: 

\begin{eqnarray}
g_{kk} & = & \lim_{n\rightarrow\infty}\left(f_{kk}\right)^{n}\label{eq:A}\\
g_{jk} & = & f_{jk}g_{kk}\nonumber 
\end{eqnarray}



\paragraph{proof:}

$g_{kk}=\lim_{n\rightarrow\infty}P\left[\mbox{every returning to \ensuremath{k}from 1th \ensuremath{k}}\cap\mbox{ever returning from the second }k\cap\ldots\cap\mbox{ever returning from the nth}k\right]$

Now condition backward. 


\subsubsection*{Theorem}

For any state $k$ either $g_{kk}=0$ or $g_{kk}=1$, further 

$g_{kk}=0\Leftrightarrow f_{kk}<1$

$g_{kk}=1\Leftrightarrow f_{kk}=1$

Proof follows directly from \ref{eq:A}

After all this background we come to en important theorem: Theorem
{[}A zero-one|aw{]}


\section{Application to Branching process}

We show that in a simple (\noun{galton-watson}) Branching process,
the successive generation size either tend to infinity with probability
$1$ or die out with probability $1$.

Let $g_{kk}$ be the prob that the chain returns infinitely often
to the state $k$ we shall show that $g_{kk}=0\,\forall k=0,1,\ldots$
that is no matter what finite set of state you choose, you will visit
these states only a finite number of times.

To proves this all we need to do is show that $f_{kk}<1$
\end{document}
