%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{luainputenc}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{slashed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\noun}[1]{\textsc{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newenvironment{lyxlist}[1]
{\begin{list}{}
{\settowidth{\labelwidth}{#1}
 \setlength{\leftmargin}{\labelwidth}
 \addtolength{\leftmargin}{\labelsep}
 \renewcommand{\makelabel}[1]{##1\hfil}}}
{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[pdftex,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2]{hyperref}

\makeatother

\usepackage{babel}
\begin{document}

\title{Introduction to Stochastic Processes\\
MATH-447 McGiil}


\author{Robin Solignac\\
MJ Lagarde}

\maketitle
\tableofcontents{}


\chapter{conditional probabilities, expectation. And probability generating
function}


\section{conditional probabilities and expectation, definitions}

Definition: let's two random variables $X$ and $Y$ have a join discrete
probability function $P(x,y)=P(X=x,Y=y)$ for $(x,y)\subset A$ (called
the support of the distribution) and $P(X=x,Y=y)=0\mbox{ for }(x,y)\notin A$.

Then the conditional probability of $Y$ given $X=x$, $P_{y/X=x}(y|x)$
is defined to be $\frac{P_{X,Y}(x,y)}{P_{X}(x)}$ for $P_{X}(x)\neq0$.
Here $P_{X}(x)$ is called the marginal. $P_{X}(x)=\sum_{\forall y}P_{X,Y}(x,y)$. 


\paragraph{Notes : }
\begin{enumerate}
\item If you fix x, then $P{}_{Y|X}(y|x)$ consider as function of Y, defines
a conditional probability distribution of Y given x.
\item There is no such thing as the random variable $Y|X=x$. It is simply
defined as $P(Y=y|X=x)$ is defined as above. When you see the statement
$Y|X=x$ don't interpret this to mean that that the r.v had the $p$
function $P_{y/X=x}(y|X=x)$. It means $P_{y|X=x}(y|x)=P(Y=y|X=x)$. 
\end{enumerate}

\paragraph{Definition 2: }

If the random variables $(X,Y)$ are jointly discrete, the conditional
expectation of $Y$ given $X=x$, written $E(Y|X=x)$ is defined as
follows : $E(Y|X=x)=\sum_{\forall y}y*P_{y/X=x}(y|x)$. 


\paragraph{Definition 3: }

If the random variables $(X,Y)$ are jointly continuous with probability
density function (pdf) $f_{X,Y}(x,y)$ then we define conditional
pdf of $Y$ given $X=x$ as follows : $f_{Y|X}=x(y|x)=\frac{fX,Y(x,y)}{fX(x)}$

Here $\intop_{-\infty}^{\infty}f_{X,Y}(x,y)\,dy$ is called marginal
pdf of $X$.


\paragraph{Definition: }

If the r.v. $X$ and $Y$ are continuous, with joint pdf $f_{X,Y}(x,y)$
then we define the conditional expectation of $Y$ given $X=x$ as
follows $E(Y|X=x)=\intop_{-\infty}^{\infty}y*f_{Y|X=x}(y|x)\,dx$


\section{The laws of total probability and total expectation}


\subsection{Law of total probability : }

Let X,Y be random variables with some join discrete distribution or
continuous distribution.
\begin{lyxlist}{00.00.0000}
\item [{(I)}] If discrete $P_{Y}(y)=\sum P_{Y|X=x}(y|x)*P_{X}(x)=\sum P(X=x,Y=y)$ 
\item [{(II)}] If continuous :$F_{y}(y)=\int f_{Y|X}(y|x)*f_{X}(x)\,dx$
\item [{(I')}] $F_{Y}(y)=P(Y\leq y)=\sum P_{y/X=x}(y\leq y|x)*P_{X}(x)=\sum F_{y/X=x}(y|x)*f_{X}(x)\,dx$
where $\sum F_{y/X=x}(y|x)=p_{Y/X=x}(zx)z\,:\,z\leq y$
\item [{(II')}] $F_{Y}(y)=\intop p(y\leq y|X=x)*f_{X}(x)\,dx$
\end{lyxlist}

\subsection{Law of total expectation and total variance}

\[
E(Y)=E_{X}\left[E_{Y|X}\left(Y|X\right)\right]
\]



\paragraph{Note:}

How do we think of $E\left(Y|X\right)$ ? 

We have not define this since the conditioning ``things'' is itself
a \emph{random variable} (we only define $E\left(Y|X=x\right)$).\\
In fact $E\left(Y|X\right)$ is itself a random variable. For each
value of $X$ we'll get a different value of $E\left(Y|X\right)$.\\
Although in more advanced probabilities $E\left(Y|X\right)$ can be
define under very general conditions. We can give a working way of
thinking of $E\left(Y|X\right)$ under slightly stranger conditions:
\begin{enumerate}
\item Define $E\left(Y|X=x\right)$ as some $g(x)$
\item Define $E\left(Y|X\right)$to be $g(X)$
\end{enumerate}
$g(X)$ is of course a random variable as $X$ is one.

We extend the Law of total expectation to a \emph{law of total variance}:
\\
if $X$ and $Y$ have some distribution then we can write 
\[
\mbox{Var}(Y)=E_{X}\left[\mbox{Var}\left(Y|X\right)\right]+\mbox{Var}_{X}\left[E\left(Y|X\right)\right]
\]



\subsection{Application}


\paragraph{Ex: }

Let $X_{1},X_{1}$be identically distributed random variable with
common mean $E(X)=\mu$ let $N$ be a non-negative integer valued
r.v, independent of $X_{1,}X_{2},\ldots$ . Then $E\left[\sum_{i=1}^{N}X_{i}\right]=E(X)E(N)=E(N)\mu$


\paragraph{Note:}

We cannot simply write $E\left[\sum_{i=1}^{N}X_{i}\right]=\sum_{i=1}^{N}E\left[X_{i}\right]$.
Since the upper limit of summation is random and nothing in probs
allow us to take the expected value inside when the sum is random 


\paragraph{Proof:}

The presence of $\geq2$ r.v: $N$ and the $X_{i}$ suggest that conditioning
one one or more of them may make thing simpler. idea: if we condition
on $N=n$ then we have a fixed upper limit sum condition and then
$E\left[\sum_{i=1}^{n}X_{i}\right]=\sum_{i=1}^{n}E\left[X_{i}\right]$.

We use law of total expectation. Let $S_{n}=\sum_{i=1}^{n}X_{i}$
we have $E\left[S_{n}\right]=E_{n}\left[E\left[S_{n}|N\right]\right]$
and we need $E\left[S_{n}|N\right]=\left.E\left[S_{n}|N=n\right]\right|_{n=N}$.
now 
\begin{eqnarray*}
E\left[S_{n}|N=n\right] & = & E\left[\sum_{i=1}^{n}X_{i}|N=n\right]\\
 & = & E\left[\sum_{i=1}^{n}X_{i}\right]\\
 & = & \sum_{i=1}^{n}E\left[X_{i}\right]\\
 & = & n\mu
\end{eqnarray*}


since $N$ is independent with all $X_{i}$.

So $E\left[S_{n}|N\right]=\left.n\mu\right|_{n=N}=n\mu$. Finally
$E\left[S_{N}\right]=E\left[N\mu\right]=\mu E\left[N\right]$


\paragraph*{Warning:}

when you condition, retain the conditioning ``event'' until you've
decide that it can be removed. In the above $g(n)=E\left[S_{N}|N=n\right]$
so $g(N)=E\left[S_{N}|N\right]$


\subsection{Wald's identity}

the following extension is important since it allows us to remove
the assumption that $N$ is independent of $X_{1,}X_{2},\ldots$ its
called \emph{Wald's identity.} It has various form, the simplest one:

Let $X_{1,}X_{2},\ldots$ be identically distributed with $E(X)=\mu$
let be $N$ a \emph{stopping rule, }that is the decision to stop summing
at $N=n$. It depend only on $X_{1,}X_{2},\ldots,X_{n}$and not on
any $X_{i},\,i>n$. Remarkably we still has 
\[
E\left[\sum_{i=1}^{N}X_{i}\right]=E(N)E(X)=E(N)\mu
\]


To clarify what is a stopping rule, 2 example, one where $N$ is one,
one where not


\subparagraph{Ex1}

Stop summing at $N=n$ if and only if $\sum_{i=1}^{n-1}X_{i}<10$
and $\sum_{i=1}^{n}X_{i}\geq10$. Stop summing at $n$ depend only
of $X_{1,}X_{2},\ldots,X_{n}$


\subparagraph{Ex2}

Example where $N$ won't be the stopping rule:

Let $X_{1,}X_{2},\ldots$ denote the of cracks fond in a successive
weeks in an aircraft component. $S_{n}=\sum_{i=1}^{n}X_{i}$ = cumulative
number of cracks up to the random week, N. Now suppose N is defined
as follows: $N=n$, if $0$ new cracks occurs in week $n+1$. With
this stopping rule, the decision to stop adding the number of cracks
at month n depends o,n what happens after week n. Thus n is not a
stopping rule. 


\section{Probability Generating Function : }

The following ``generating function'' plays a role similar to moment
generating function, but it's definition is restricted to non-negative
integer valued r.v.s, whose support is on \{0,1,2..\}.


\paragraph{Definition }

Let X be a discrete r.v. with support on \{0,1,2..\}. The probability
generating function (pgf) is defined as follows: 
\[
\Phi_{X}(z)=\sum_{x=0}^{\infty}z^{x}p_{x}(x)=\sum_{x=0}^{\infty}z^{x}P\left[X=x\right]
\]
 for $|z|\leq1$

Notes: 
\begin{enumerate}
\item $\Phi_{X}(z)$ is a power series in z which converges for |z|$\leq1$
\item $\Phi_{X}(1)=1$
\item Reason for the name : Given $\Phi_{X}(z)$ one can generate or recover
$p_{x}(x)$ for all $x$. That is there is a one to one correspondence
between a pgf and its distribution. \\
Proof: Since $\Phi_{X}(z)$ is a power series in z it is uniformly
convergent for $|z|<1$. Hence $\Phi'_{x}(z)$ exists for such z and
$\Phi'_{x}(z)=\frac{d}{dz}\sum_{0}^{\infty}p_{x}(x)=\sum_{0}^{\infty}xz^{x-1}p_{x}(x)=\sum_{1}^{\infty}xz^{x-1}$
$\Phi_{x}'(0)=p_{x}(1)$\\
In a similar fashion, by differentiating successively, setting z=0,
we get $\Phi_{x/2!}^{"}(0)=\frac{p_{x}(z)}{z!}$ to give $p_{x}(x)$=$\Phi_{X}^{x}(0)/x!$
or in a more familiar notation : $P_{X}(k)$= $\Phi_{X}^{k}(0)/k!,$
for k=1.. with $\Phi_{X}^{0}(0)=p_{X}(0)=\Phi_{X}(0)$
\item By setting $z=e^{t}$ we obtain the mgf $\sum_{x}e^{tx}p_{x}(x)$
\item $\Phi_{X}(z)=E[z^{X}]$
\item It then follows from 5 that if $X_{1,}X_{2},\ldots,X_{n}$ are independents
r.v.s, then $\Phi_{\sum_{i=1}^{n}}(z)=E[z^{\sum_{1}^{n}Xi}]=E[\prod_{1}^{n}z^{Xi}]=\prod_{i=1}^{n}E[z^{Xi}]$=$\prod_{i=1}^{n}\Phi_{xi}[z]$
further if the Xiw are identically distributed then $\Phi_{\sum_{i=1}^{n}}(z):[\Phi_{x}(z)]^{n}$
\end{enumerate}

\chapter{Stochastic processes themselves}


\section{Definition }

A stochastic process indexed by an index set $T$ is a family of random
variables, denoted by $\left\{ X{}_{t},\,t\in T\right\} $ such for
each $t\in T$, $X{}_{t}$ is a random variable.


\paragraph{Notes}
\begin{enumerate}
\item For each $t\in T$, $X{}_{t}$ is a r.v. means $X{}_{t}=X{}_{t}(\omega)$
is a function of $\omega\in S,$ the sample maps of the possible outcomes
of $X{}_{t}$ . That is for each t, $X{}_{t}$ had a probability distribution
that is specified by $F_{X{}_{t}}(.)$its cdf. 
\item For each fixed $\omega,$ $X{}_{t}$ is a function of $t\in T.$(the
idkr set). We call this function of t (for fixed $\omega\epsilon S$)
a trajectory or sample path of stochastic process. We may depict the
situation as follows. 
\item How is a stochastic process specified? Recall that if X is a r.v.
it is completely determined or specified once you have specified c.d.f
$F_{X}$. In the case of stochastic process it turns out that it is
uniquely specified once you have specified $F_{Xt1},$$F_{Xt2},$...,
$F_{Xtn}$ for all the collections of the r.v.s $X_{t1},$$X_{t2},..,X_{tn}$for
all $t1,t2,..,tn\in T$, and all $n$. 
\end{enumerate}

\paragraph{Notes:}

we can view stochastic process as follow, since $\omega\in S$ is
unknown in advance, each projectors in unknown in advance of our experiment.
Therefore, we can regard a stochastic process as a random function.
We are sitting in a function space. Each point in this space is a
function. Question that we may ask one ``What is the probability
that this function will fall in some set in this function space?

Extensen theorem is that the probabilities of all the event in this
function space is uniquely determined by the probabilities specified
by the finite themselves c.d.f.S.

Examples stochastic process:
\begin{enumerate}
\item Trivially any single r.v., $X$ is a stochastic process - the family
r.v.
\item the sequence of random variable $X_{1},X_{2},X_{3}$...
\end{enumerate}
recall knowing $P\left[X_{t1}\leq x_{1},\ldots,X_{tn}\leq X_{n}\right]=F_{X_{t1},\ldots,X_{tn}}^{(x_{1},\ldots x_{n})}\,\forall ti\forall n\forall-\infty<x_{i}<\infty$
gives the unique probabilities $P\left[\left\{ X_{t}\in B,t\in T'\right\} \right]$
(notes: this is useless)


\paragraph{Note:}

The join finite dimensional distribution are uniquely determined in
the case of a sequence of i.i.d r.v.s by the marginal distribution
$F_{X}(x)$ of any member of the sequence. : $F_{X_{t1},\ldots,X_{tn}}^{(x_{1},\ldots x_{n})}=\prod_{i=1}^{n}F_{X}(x_{i})$

Such sequences are not really the subject of study in stochastic processes
because of the assumption of independence


\section{Some non trivial example of stochastic processes}
\begin{enumerate}
\item $\left\{ x_{t},\,t\in D\right\} $where $X_{t}$ si the price of e
certain stock during day $D$. before the stock market opens. the
price over the day are uncertain. the trajectory of $X_{t}$ will
typically fluctuate and the $X_{i}$'s will be dependent.
\item $\left\{ X_{t},\,i\in\left\{ 1,2,\ldots\right\} \right\} $ where
$X_{t}$ denote the number of bacteria on e petri dish at minute $t$
from the time they start dividing
\item $\left\{ X_{t},\,i\in\left\{ 1,2,\ldots\right\} \right\} $ where
$X_{t}$ is the height of a dam above level $L$ at time $t$ 
\item if $\left\{ X_{1},X_{2},\ldots\right\} $is a sequence of i.i.d r.v.s,
let $S_{n}=\sum_{i=1}^{n}X_{i}$ then $\left\{ S_{n},\,n=1,2,\ldots\right\} $
is a stochastic process and is called a \emph{random walk.}\\
Note: while $X_{i}$s are independent, the $S_{n}$ are highly dependent
\end{enumerate}

\section{Jargon}
\begin{enumerate}
\item if index set is discrete we refer to a \emph{discrete parameter} stochastic
process\\
If continuous ($T$ uncountable) we refer to a \emph{continuous parameter}
stochastic process
\item If the set of possible value of $X_{t}$ countable (discrete) we refer
to a \emph{discrete state space}\\
If uncountable we refer to a \emph{continuous state space}
\end{enumerate}

\chapter{Branching Processes}

We will begin by formally defining it, and then mention some application


\section{Definition}

Let $\left\{ X_{n},n=0,1,2,\ldots\right\} $ be a stochastic process
of the following properties
\begin{enumerate}
\item $X_{0}=1$
\item the probability that this individual ($X_{0})$ gives rise to $j$
individuals at the next generation is $p_{j}$, for $j=0,1,2,\ldots$\\
i.e $P\left[X_{1}=j\,|\,X_{0}=1\right]=p_{j}\,\forall j$
\item \emph{Each} individual in the 1\textsuperscript{st }generation gives
rises to $j$ new individuals in the 2\textsuperscript{nd}generation
with probability $p_{j}$, for $j=0,1,2,\ldots$. Independently of
all other individuals in this generation and of the generation number
($n$), and so on for subsequence generations
\end{enumerate}
Let $X_{n}$ be the total size of the n\textsuperscript{th}generation
(not the cumulative sum over generation). Then $\left\{ X_{n},n=0,1,2,\ldots\right\} $is
called a \emph{branching process} (or Galton-Watson branching process)


\paragraph{Example:}
\begin{itemize}
\item Physics: nuclear reaction
\item Biology: occurrence of mutant gene over generation, survival of family
name
\end{itemize}

\section{Question we shall address}
\begin{enumerate}
\item what are the probability of eventual extinction of the population
?
\item What are then expected population size of the n\textsuperscript{th}generation
? The variance ?
\end{enumerate}
To answers 1. we shall need to work quite hard and we end up with
an interesting, perhaps unexpected, result. 

Even though we can view $\left\{ X_{n},n=0,1,2,\ldots\right\} $ as
a Markov chain, we shall answers these questions without reference
to them by using conditional expectation and probability generating
function. 

The details are important !!


\section{Answers:}

Let $p_{0}(n)$ denote the probability that there are no individuals
in the n\textsuperscript{th} generation. We want to find 
\begin{eqnarray}
P\left[\bigcup_{k=1}^{\infty}\left\{ X_{k}=0\right\} \right] & = & P\left[\lim_{n\rightarrow\infty}\bigcup_{k=1}^{n}\left\{ X_{k}=0\right\} \right]\nonumber \\
 & = & \lim_{n\rightarrow\infty}P\left[\bigcup_{k=1}^{n}\left\{ X_{k}=0\right\} \right]\label{eq:lim}
\end{eqnarray}


Now notes that $X_{n}=0\Rightarrow X_{n+1}=0$ . i.e becomes (\ref{eq:lim})
\[
\lim_{n\rightarrow\infty}P\left[X_{n}=0\right]=\lim_{n\rightarrow\infty}p_{0}(n)
\]


Let $\Phi_{X_{n}}(z)$ be the p.g.f of $X_{n}$, for any p.g.f we
have $\Phi(0)=p_{0}\Rightarrow p_{0}(n)=\Phi_{X_{n}}(0)$.

Thus it seems that the discussion reduce to one of p.g.f.s. this is,
indeed, the case. Let $\Phi(z)$ be the p.g.f of $X_{1}$, which of
course is the p.g.f of the r.v that gives the number that arise from
any single of individuals in\emph{ any} generation


\paragraph{Lemma:}

\begin{eqnarray}
\Phi_{X_{n}}(z) & = & \Phi_{X_{n-1}}\left(\Phi(z)\right)\label{eq:11}\\
 & = & \Phi\left(\Phi_{X_{n-1}}(z)\right)\;\forall n\geq1\label{eq:12}\\
\nonumber 
\end{eqnarray}


The idea in proving both (\ref{eq:11}) and (\ref{eq:12}) is that
in each case we can write $X_{n}$ as a random sum of random variable.
with this observation we shall condition the upper limit of summation.


\subsection{Proof of the lemma }


\subsubsection{Proof of (\ref{eq:11})}

Observe that if we let $Y_{i}$ be the number of ``offspring'' from
parent $i$ in the (n-1)\textsuperscript{th}generation, then $X_{n}=\sum_{i=1}^{X_{n-1}}Y_{i}$


\subparagraph{Note:}

that we haven't not attached a subscript $n-1$ to $Y_{i}$ (i.e $Y_{i,n-1}$)
since the distribution of $Y_{i,n-1}$ is the same for all $n$ by
the assumption of branching processes. 

We now need $\Phi_{X_{n}}(z)=\Phi_{\sum_{i=1}^{X_{n-1}}Y_{i}}(z)$.
Looks bad, but observe that if we condition on $X_{n-1}=k$, than
we would be dealing with $\Phi_{\sum_{i=1}^{k}Y_{i}}(z)$ the p.g.f
of a fixed sum of i.i.d r.v.s. The $Y_{i}$'s are independent and
identically distributed by assumption.

Thus use $\Phi_{X_{n}}(z)=E\left[z^{X_{n}}\right]=E_{X_{n-1}}\left[E\left[z^{\sum_{i=1}^{X_{n-1}}Y_{i}}|X_{n-1}\right]\right]$

now: 
\begin{eqnarray*}
E\left[z^{\sum_{i=1}^{X_{n-1}}Y_{i}}|X_{n-1}=k\right] & = & E\left[z^{\sum_{i=1}^{k}Y_{i}}|X_{n-1}=k\right]\\
 & = & E\left[z^{\sum_{i=1}^{k}Y_{i}}\right]\mbox{indep of \ensuremath{\left\{ X_{n-1}=k\right\} }and \ensuremath{\left\{ Y_{1},\ldots,Y_{k}\right\} }}\\
 & = & \Phi_{\sum_{i=1}^{k}Y_{i}}(z)\\
 & = & \left[\Phi_{X_{1}}(z)\right]^{k}=\left[\Phi(z)\right]^{k}
\end{eqnarray*}
Note that's because$Y_{i}$ as the same distribution as $X_{i}$.

Finally $E_{X_{n-1}}\left[\left(\Phi(z)\right)^{X_{n-1}}\right]=\Phi_{X_{n-1}}\left(\Phi(z)\right)$
(by the definition of $\Phi_{X_{n-1}}$).

this proves (\ref{eq:11})


\subsubsection{Proof of (\ref{eq:12})}

to get (\ref{eq:12}) we we argue slightly differently. The no in
the n\textsuperscript{th}generation is the sum The sum of the number
that come from the sub generation of each ``parent'' in the first
generation.

The number that end up $(n-1)$ generation after each parent in the
1\textsuperscript{st}are i.i.d r.v.s, Call them $X_{n-1,i}\,i=1,\ldots,X_{1}$.
Thus $X_{n}=\sum_{i=1}^{X_{1}}X_{n-1,i}$. 

Using the same type of argument, as for (\ref{eq:11}) (Conditioning
on $X_{1}$ this time), we get 
\begin{eqnarray*}
E\left[z^{X_{n}}\right] & = & E_{X_{1}}\left[E\left[z^{\sum_{i=1}^{X_{1}}X_{n-1,i}}|X_{i}\right]\right]\\
 & = & \Phi_{X_{1}}\left[\Phi_{X_{n-1}}(z)\right]=\Phi\left[\Phi_{X_{n-1}}(z)\right]
\end{eqnarray*}


These equality have been established for all $n\geq1$ and are easily
seen (do this) to be true for $n=1$


\subsection{Answer of 1.}

(\ref{eq:11}) and (\ref{eq:12}) form the basis for the completion
of the analysis :

\[
\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)=\lim_{n\rightarrow\infty}\Phi_{X_{n-1}}\left(\Phi(0)\right)=\lim_{n\rightarrow\infty}\Phi\left(\Phi_{X_{n-1}}(0)\right)
\]


We still examine $\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)$, but
first we need to justify the existence of this limit.


\paragraph{Note:}

that $\Phi_{X_{n}}(z)$ is bounded for all $|z|\leq1$ and for all
$n$ $\Phi_{n}(z)\leq\sum_{k=0}^{\infty}|z|^{k}P\left[X_{n}=k\right]\leq\sum_{k}P\left[X_{n}=k\right]=1$.

Since $\Phi_{X_{n}}(0)=P\left[X_{n}=0\right]$ and since $\left\{ X_{n}=0\right\} \subset\left\{ X_{n+1}=0\right\} $
then$P\left[X_{n}=0\right]\leq P\left[X_{n+1}=0\right]$

And so$\left\{ \Phi_{X_{n}}(0)\right\} $is a bounded non decreasing
sequence and thus $\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)$ exist
$\eta$.


\paragraph{Recall:}

$\Phi_{X_{n}}(z)=\Phi_{X_{n-1}}\left(\Phi(z)\right)=\Phi\left(\Phi_{X_{n-1}}(z)\right)\;\forall n\geq1$.
(\ref{eq:11}) then (\ref{eq:12})

And so using using (\ref{eq:12}) $\lim_{n\rightarrow\infty}\Phi_{X_{n}}(0)=\lim_{n\rightarrow\infty}\Phi\left(\Phi_{X_{n-1}}(0)\right)=\Phi\left(\lim_{n\rightarrow\infty}\Phi_{X_{n-1}}(0)\right)$
($\Phi$ is continuous). this implies $\eta=\Phi(\eta)$ so the limit
must satisfy this properties.

We need to examine the root of $z_{0}=\Phi(z_{0})$.

It's easy to see that $\Phi(z)$ is convex in $z$. so differentiate
twice.

For now on, assume that $P\left[X_{1}=0\right]>0$, if not the probability
of extinction will be $0$. We have the following pictures (non)

We see that $z=1$ is a root since $\Phi_{X_{n}}(1)=1$. We see that
there's 2 possible roots.

We shall see that $\eta$ is the smallest possible root. 

We have $p_{0}(1)=\Phi(0)\leq\Phi(z_{0})=z_{0}$ for any possible
root $z_{0}$ of $\Phi(z)=z$. Using (\ref{eq:11})$p_{0}(z)=\Phi_{X_{2-1=1}}\left(\Phi(0)\right)=\Phi(z_{0})=z_{0}$.
and so on for all $p_{0}(n)$ 

So: $\lim_{n\rightarrow\infty}p_{0}(n)=\eta\leq z_{0}$. And so $\eta$
is the smallest positive root of $\Phi(z)=z$.


\paragraph{Question:}

Under what circumstance is there only one positive root ? that is,
when will the only root be $\eta=1$.

From the pictures we see that there's exactly one root (at $z=1$)
if and only if $\Phi'(1)\leq1$. But $\Phi'(1)=\mu=E\left(X_{1}\right)$


\paragraph{So:}

the branching process will end in extinction if $\mu\leq1$ we will
have a probability of extinction $\eta>0$ if $\mu>1$ (i.e there's
is pos probability of non-extinction). note that is never the case
that $\eta=0$ : $\mu>1\Rightarrow\eta>0\wedge\eta<1$


\paragraph{Notes:}
\begin{enumerate}
\item In practice if a population is to become extinct, this happens quickly
\item if we start with $a$ individuals (instead of $1$) then the probability
of extinction of \emph{all} descendant is $\eta^{a}$ (since the lines
develops independently)\\
thus, with $a$ to star with, the probability of non extinction is
$1-\eta^{a}$


This has interesting application in nuclear fission (in nuclear chain
reaction) because obviously we want the reaction (branching process)
to die out


Even if $\eta$ is large, $\eta^{a}$ will be small if $a$ (the ``critical
mass'') is taken to be \emph{very} large, in this case $\eta^{a}$
will be small and the probability that the nuclear reaction will be
maintained $1-\eta^{a}$ will be large

\item one extension to the Glaton branching-process is the so called Branching
process with Random Environment. the ``division probability'' $p_{j}$
change over time. For example, the environment might changing (is
it !) leading to different reproduction rates
\item Another generalization is to Multi-type Branching Process. Whereby
a population is composed of different subtype each with a different
$\left\{ p_{j}\right\} $
\end{enumerate}

\subsection{What is $E\left(X_{n}\right)$ and $\mbox{Var}\left(X_{n}\right)$
? (Answer of 2.)}

Let $\mu_{n}=E\left(X_{n}\right)$. Then $\mu_{n}=\Phi'_{X_{n}}(1)$.
But we know that $\Phi_{X_{n}}(z)=\Phi_{X_{n-1}}\left(\Phi(z)\right)$(\ref{eq:12})

Then we have $\mu_{n}=\Phi'_{X_{n}}(z)=\Phi'(1)\Phi_{X_{n-1}}'(1)$
(chain rule)

Which mean that $\Phi'_{X_{n}}(z)=\mu_{n}=\mu\Phi'_{X_{n-1}}(1)=\mu\mu_{n-1}$
which gives immediately $\mu_{n}=\mu^{n}$

Thus is $\mu>1$ the mean population grows geometrically , and if
$\mu<1$ it decline very quickly (geometrically)), If $\mu=1$ then
the population size is constant ($=1$).

To obtain $\mbox{Var}\left(X_{n}\right)$ we use (\ref{eq:12}) once
more (see 2) bur argument is more difficult. We find: 

\begin{eqnarray*}
\mbox{Var}\left(X_{n}\right)=\sigma_{n}^{2} & = & \sigma^{2}\mu^{n-1}\frac{\left[\mu^{n}-1\right]}{\mu-1},\mbox{ if \ensuremath{\mu\neq1}}\\
 & = & n\sigma^{2}\mbox{, if \ensuremath{\mu=1}}
\end{eqnarray*}
Thus $\sigma_{n}^{2}$increase (decrease) geometrically if $\mu\neq1$,
possibly explaining why $\eta>0$ even if $\mu>1$: large fluctuation
give a chance of falling into the breach of extinction.


\chapter{Markov Chains}

Almost always there is an underlying dependence structure to the stochastic
processes that we discuss. A special type of dependence structure
is assumed when we discuss Markov chain, this structures assumes the
so calls \emph{Markov property} 

To begin, we shall assume that our process$\left\{ X_{n},\,n=0,1,\ldots\right\} $
is a discrete parameter countable state space process. The state of
space of $\left\{ X_{n},\,n=0,1,\ldots\right\} $ could be denoted
$E_{1,}$$E_{2}..$ but for notation simplicity, by, $1,2,\ldots$


\section{Definition of a Markov chain}


\paragraph{Definition:}

$\left\{ X_{n},\,n=0,1,\ldots\right\} $ is called a (discrete state
space) Markov chain if

$P\left[X_{n+1}=j|X_{0}=i_{0},X_{1}=i_{1},...,X_{n}=i\right]=P\left[X_{n+1}=j|X_{n}=i\right]=P_{ij}$
for all $n\geq0$, and all states $i_{0},i_{1},\ldots,i_{n}$


\paragraph{Notes:}

1) $P_{ij}$is called the one step transition probability of the chain.
It describes the probability of moving from at state $i$ to a state
$j$ in one state. 

Notice that $P_{ij}$ is assumed not to depend on n. 

i.e. we should really write $P_{ij}^{n}$. Chain for which $P_{ij}$
does not depend on $n$ are called \emph{homogeneous}. {[}If depend
on n, non-homogeneous{]}. We restrict ourselves to homogenous chains. 

2) Importantly, the Markov property says that the history is summarized
in the immediate past. 

It does not assert that $X_{n+1}$is independent of $X_{0},X_{1},\ldots,X_{n-1}$

Example: Our branching process is easily seen to be a Markov chain,
since given the generation size, $X_{0},X_{1},..,X_{n-1},X_{n}$ the
distribution of $X_{n+1}$depends only on $X_{n}$

4)The term homogeneous is sometimes called stationary i.e. we talk
of a stationary Markov Chain. 

Reason(to be further clarified later) is that with the assumption
of homogeneity (stationarity) the join finite dimensional distributions,
are invariant under shifts in time.


\subparagraph{Example of non-stationary Markov chain:}

A branching process with the number of children from each from each
parent in a generation, having a distribution that depends on the
generation number in fertility changes with n. 

5) a process that satisfies the Markov property above is called 1st
order Markov. If the conditional probability reduced to what the previous
2 states were, we called such a process 2nd order Markov.. etc. 


\section{How does all of this work? }

We wish to model a sequence of dependent r.v.s. To do this requires
specifying the joint finite dimensional distribution - a hard problem.
However, if we can assume the special dependence structure induced
by the Markov property then we'l see that the specification of the
finite dimensional distribution is make much simpler. Of course, one
has to be satisfied that the Markov property holds. There are statistical
tests that can be used to test the Markov assumption. 

Terminology:

The one step transmission probabilities are denoted by$P_{ij}$. The
complete set of $P_{ij}$ is displayed in the transition matrix:

$P=\begin{bmatrix}p_{11} & p_{12} & \cdots\\
p_{21} & p_{22} & \cdots\\
\cdots & \cdots & \ddots
\end{bmatrix}$

Some $P_{ij}$ may be 0. We have immediately that $\sum_{j}P_{ij}=1$
for all $i$. You have to go some-where from state i. $P$ is called
a stochastic matrix - non negative entries sum to one. Of course the
matrix could be finite corresponding to $m$ states. 

Precision: $P_{ij}^{(n)}=P\left[X_{n}=j|X_{0}=i\right]$ = Probs of
going from $i$ to $j$ in $n$ steps

The aim here is to find an expression for $P_{ij}^{(n)}$, or at least
a mean of obtaining $P_{ij}^{(n)}$. In principle, the way is through
what we call the \noun{Chapman-Kolmogorov} equations


\subsection{Theorem:}

let $P^{(n)}=\left\{ p_{ij}^{(n)}\right\} $ be the matrix whose entries
are the n-step transition probabilities $p_{ij}^{(n)}$. Then 
\[
P^{(m+n)}=P^{(m)}P^{(n)}
\]


a corollary will be 
\[
P^{(n)}=P^{n}=\left[\left\{ p_{ij}\right\} \right]^{n}
\]



\subsection{Proof}


\paragraph{Theorem proof:}

\begin{eqnarray*}
P\left[X_{m+n}=j|X_{0}=i\right] & = & \sum_{k}P\left[X_{m+n},X_{m}=k|X_{0}=I\right]\\
 & = & \sum_{k}\frac{P\left[X_{m+n}=j,\,X_{m}=k,\,X_{0}=i\right]}{P\left[X_{0}=i\right]}\\
 & = & \sum_{k}\frac{P\left[X_{m+n}=j|X_{m}=k,\,X_{0}=i\right]P\left[X_{m}=k|X_{0}=i\right]P\left[X_{0}=i\right]}{P\left[X_{0}=i\right]}\\
 & = & \sum_{k}\underbrace{P\left[X_{m+n}=j|X_{m}=k\right]}_{\mbox{Markov prop}}P\left[X_{m}=k|X_{0}=i\right]\\
 & = & \sum_{k}\underbrace{P_{ik}^{(m)}p_{kj}^{(n)}}_{\mbox{Homogenity }}=P_{ij}^{(m+n)}
\end{eqnarray*}


Which shows that $P_{ij}^{(m+n)}$ is just the i-j\textsuperscript{th}elements
of the product of the matrices $P^{(m)}$ and $P^{(n)}$, i.e: $P^{(m+n)}=P^{(m)}P^{(n)}$


\paragraph{Corollary proof:}

In particular take $m=1,\,n=1,\,\left[n+m=2\right]$ then $P^{(2)}=P.P=P^{2}$
and then $P^{(3)}=P.P^{(2)}=P.P^{2}$ , etc\ldots{} This can be generalize
by induction to $P^{(n)}=P^{n}$

In principle if the chain finite (it has finite many step) we can
find $P_{ij}^{(n)}$ by doing successive matrix multiplication.

If the number of step is infinite then this does not work and we will
se that w often end up having to approximate $P_{ij}^{(n)}$ for large
$n$ by $\lim_{n\rightarrow\infty}P_{ij}^{(n)}$ (Still to come)


\subsection{Some example.}


\subsubsection{Example 1}

Consider a communication system which transmit the digits 0 or 1.
Each digit transmitted must pass through serval stage at each of which
there's a prob $p$ that the digit will be unchanged when it leaves
the stage, let $X_{n}$ the digits the $n^{th}$ stage then $\left\{ X_{n},n=0,1,\ldots\right\} $
is a 2-state Markov chain with $P=\left[\begin{array}{cc}
p & 1-p\\
1-p & p
\end{array}\right]$


\subsubsection{Example 2 The simple random Walk}

This is a basic well-studied non-trivial example of a Markov chain
that is constructed from basic properties easy to imagine.

Suppose that a particle can jump between the integers ($\mathbb{Z}$)
on the x-axis according to the following rules
\begin{enumerate}
\item It can move either form it current position (integer) by moving either
1 step to the left, or 1 step to the right
\item it moves independently of how many chain it has taken
\item it move 1 step to right with probability $p$ no matter what is its
current position, and 1 step to the left with prob $1-p$
\item it moves independently of its previous move 
\end{enumerate}
Let $X_{n}$ be the position of the particle after $n$ moves. we
call $\left\{ X_{n},\,n=0,1,2,\ldots\right\} $ the \emph{simple random
walk} 


\paragraph{Claim:}

$\left\{ X_{n},\,n=0,1,2,\ldots\right\} $ form a Markov chain.

We begin by writing down the transition matrix 
\[
P=\left[\begin{array}{ccccccc}
\ddots & \cdots & \cdots & \cdots & \cdots & \cdots & \vdots\\
\vdots & 0 & p & 0 & 0 & 0 & \vdots\\
\vdots & 1-p & 0 & p & 0 & 0 & \vdots\\
\vdots & 0 & 1-p & 0 & p & 0 & \vdots\\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \ddots
\end{array}\right]
\]
(This matrix is centered on 0).

Next to show that $\left\{ X_{n},\,n=0,1,2,\ldots\right\} $ is a
Markov chain, write $X_{n}$ as $X_{n}=\sum_{l=0}^{n}y_{l}$ where
$y_{l}$ are r.v.s such that $P\left[y_{l}=+1\right]=p,\,P\left[y_{l}=-1\right]=1-p$. 

These $y_{l}$ are the step sizes, $X_{n}$ is a result of summing
the steps to the right plus to the left. Now let's check the Markov
properties

\begin{eqnarray*}
P\left[X_{n+1}=j\,|\,X_{0}=0,X_{1}=i_{1},\ldots,X_{n}=i\right] & = & P\left[\sum_{l=0}^{n+1}y_{l}=j\,|\,X_{0}=0,\sum_{l=0}^{1}y_{l}=i_{i},\ldots,\sum_{l=0}^{n}y_{l}=i\right]\\
 & = & P\left[\sum_{l=0}^{n}y_{l}+y_{n+1}=j\,|\,X_{0}=0,\ldots,\sum_{l=0}^{n}y_{l}=i\right]\\
 & = & P\left[\left(i+y_{n+1}\right)=j\,|\,\ldots\right]\\
 & = & P\left[y_{n+1}=j-i\,|\,X_{0}=0,\ldots,\sum_{l=0}^{n}y_{l}=i\right]\\
 & = & P\left[y_{n+1}=j-i\right]
\end{eqnarray*}


Because in fact because $y_{n+1}$ is independent of all the sums
to the n\textsuperscript{th} by assumption (3). But $P\left[y_{n+1}=j-i\right]=P\left[\sum_{l=0}^{n+1}y_{l}=j\,|\,\sum_{l=0}^{n}y_{l}=i\right]$
which establishes the Markov property.

Further, $P\left[y_{n+1}=j-i\right]=\begin{cases}
p & j=i+1\\
1-p & j=i-1\\
0 & \left|j-i\right|>1
\end{cases}$


\subsubsection{Example 3}

Consider a gambler who at each play wins \$1 with prob $p$ and loses
\$1 with prob $1-p$. Suppose that the gambler stop playing when she
goes broke or attains \$$N$.

Let $X_{n}$ be the gambler fortune after $n$ games. We claim that
$X_{n},\left\{ n=0,1,\ldots\right\} $ is a Markov chain with what
one known as absorbing state at $0$ and $N$: once you are at $0$
or $N$ you remain there with prop $1$ for all $i$. You have to
go some-where from state $i$.


\section{Communicating states and classification of states}


\subsection{Finite dimensional distributions}

$P$ is called a stochastic matrix - non negative entries sum to one.
Of course the matrix could be finite corresponding to $m$ states.
The following theorem says that the knowing $P[X_{0}=i]$ for $i=1,2,...${[}the
so called initial distribution{]} and the transition matrix $P$ is
sufficient to completely specify the markov chain.

Let $\left\{ X_{n},n=0,1..\right\} $ be a Markov chain. Then, its
finite dimensional distributions are uniquely determined by $\left\{ P[X_{0}=i],i=1,2,..,P\right\} $


\subsubsection*{Proof:}

We must know that we can write the joint distribution of any set $X_{n1},X_{n2}....$
in term of $P[X_{0}=i]$ and a ``bunch'' of i-step transition probabilities
$P_{ij}$

We convey the idea through a special case: Thus consider $P[X_{2}=k,X_{3}=l]$

Thus 
\begin{eqnarray*}
P[X_{2}=k,X_{3}=l] & = & P[X_{3}=l|X_{2}=k]*P[X_{2}=k]\\
 & = & P\ensuremath{_{kl}*\sum_{i,j}P[X_{2}=k\cap X_{0}=i,}\ensuremath{X_{1}=j]}\\
 & = & P_{kl}*\sum P[X_{2}=k|X_{1}=j]*P[X_{i}=j|X_{0}=i]*P[X_{0}=i]\\
 & = & P_{kl}\sum_{i,j}P_{ij}P_{jk}P[X_{0}=i]
\end{eqnarray*}
which is a function only of the i-step transition probs and $P[X_{0}=i]$.

The idea is simply that to find the probabilities of going from state
$i$ to state $j$ in $n$ steps you sum the probabilities of all
paths that take you from $i$ to $j$ in $n$ steps. Next the prob
of each path is the prob of an intersection of the form 
\begin{eqnarray*}
P\left[A_{1}\cap A_{2}...A_{n}\right] & = & P\left[A_{n}|A_{1}\cap A_{2}\ldots A_{n-1}\right]*P\left[A_{1}\cap A_{2}\ldots A_{n-1}\right]\\
 & = & P\left[A_{n}|A_{n-1}\right]*P\left[A_{n-1}|A_{n-2}\right]*\ldots*P\left[A_{1}\right]
\end{eqnarray*}


Let$P_{ij}^{n+m}$be $P[X_{m+n}=j|X_{0}=i]$

It is easy to see that the Markov property is satisfied (check this
formality).

In this case we have : 

-- 0- 1- 2- .... N 

0 1 - 0- 0- 0- 0 

1 1-p - 0 - p ...

2 0- 1-p - 0- p 


\subsection{The classification of states}

Eventually we'll be studying the limiting behavior of $P_{ij}^{(n)}$as
$n\rightarrow\infty$. In order to do so require the classification
of the states of a Markov chain and the classification of the chain
themselves


\subsubsection*{Definition: }

Let $\left\{ X_{n},n=0,1,2,..\right\} $ be a Markov chain with transmission
matrix $P=\{P_{ij}\}.$Then we say that the state $j$ is accessible
from the state $i$, if and only if there exist an $n>0$ s.t. $P_{ij}^{(n)}>0$.
{[}i.e. there is a positive prob of reaching state j from state i
in a finite number of steps{]}

Note that $P_{ij}^{(n)}$is not the probability of reaching j from
i for the first time in n steps. 

We write $i\rightarrow j$ to mean ``j is accessible from i''


\paragraph{Definition: }

If $i\rightarrow j$ and $j\rightarrow i$ then we say that $i$ and
$j$ \emph{communicate}, and write $i\leftrightarrow j$


\paragraph{Theorem:}

If $i\leftrightarrow j$ and $j\leftrightarrow k$, then $i\rightarrow k$
and conversely


\subparagraph{Corollary: $i\leftrightarrow j\wedge j\leftrightarrow k\Rightarrow i\leftrightarrow k$}

Proof: $i\rightarrow j\Rightarrow\exists n_{0}>0$ such that $P_{ij}^{(n)}>0$
and $j\rightarrow k\Rightarrow\exists n_{1}$ such that $P_{jk}^{(n)}>0$

Let $n=n_{0}+n_{1}$

Then $P_{ik}^{(n)}=P_{ik}^{(n_{0}+n_{1})}=\sum_{l}P_{il}^{(n_{0})}*P_{lk}^{(n_{1})}\geq P_{ij}^{(n_{0})}*P_{jk}^{(n_{1})}>0$

$i\rightarrow k$ done!

The corollary is immediate same $i\leftrightarrow j\Rightarrow j\leftrightarrow j$


\subsubsection{Definition of states}

Given a state j of a Markov chain, its communicating class $C{}_{j}$
is defined to be the set of all state that communicate with $j$,
in the sense that $P_{jk}^{(n)}>0$ for some $n_{0}>0$ $P_{kj}^{(n)}>0$
for some $n_{1}>0.$ It may happen that $C_{j}$ is empty {[}i.e.
j communicates with no state, not even with itself{]}. In this case
we say that $\left\{ j\right\} $ is a non-return state. 

If $C_{j}$ is non-empty, then, $j\in C_{oj}$, to see this note that
$C_{j}$ non empty implies that there must exist a.k such that $j\rightarrow k$
(and $k\rightarrow j$)

$\exists n_{0}\,:\,P_{jk}^{(n0)}>0$ and $\exists n_{1}\,:\,P_{kj}^{(n1)}>0$.
Let $n=n_{0}+n_{1}$. Then $P_{jj}^{(n)}\geq P_{jk}^{(n0)}P_{kj}^{(n1)}>0$.
$j$ communicates with itself, and this $j\in C_{j}$

A state which communicates with itself is called \emph{return state}.
A non-empty class, $C$ of state in a Markov chain is to be a \emph{communicating
class} if for some state $j$ s.t $C_{k}=C$ . That is $C$ is the
class of all states that communicate with one-another


\subsubsection*{Theorem: }

If $C_{1}$and $C_{2}$are communicating classes, then either $C_{1}=C_{2}$or
$C_{1}\cap C_{2}=\slashed{O}$


\subsubsection*{Proof: }

Let $C_{1}\cap C_{2}\neq\slashed{O}$ such that $C_{1}\neq\slashed{O}$
and $C_{2}\neq\slashed{O}$ so $\exists i\in C_{2}\,\exists j\in C_{1}\cap C_{2}$
but $C_{1}\cap C_{2}\subset C_{1}$so $i\leftrightarrow j$

\begin{eqnarray*}
\exists i\in C_{1}\\
\exists j\in C_{1}\cap C_{2}\\
\exists k\in C_{2}\\
\mbox{but}\\
C_{1}\cap C_{2}\subset C_{1}\\
C_{1}\cap C_{2}\subset C_{2}\\
\therefore i\rightarrow j\\
\therefore j\rightarrow k\\
\therefore i\rightarrow k & \mbox{transitivity of "accessibility"}\\
\therefore C_{1}\subset C_{2}\wedge C_{2}\subset C_{1}\\
\therefore C_{1}=C_{2}
\end{eqnarray*}



\subsection{Decomposition of a Markov chain into disjoint classes}


\subsubsection*{Theorem: }

The set $S$ of states of a Markov chain can be written as the union
of a finite or countably infinite family $\left\{ C_{r}\right\} $of
disjoint sets of states. $S=C_{1}\cup C_{2}...\cup C_{r}$ with $C_{i}\cap C_{2}=\slashed{O}$

Each class $C_{r}$ is either a communicating class or contains exactly
one non return state 


\subsubsection*{Proof }

Choose a state $j_{1}$ and let $C_{1}=C_{j_{1}}$, the class of all
the states that communicate with $j_{1}$ according as $j_{1}$ is
a return or non-return state, if $j_{1}$ is a non-return state, then
set $C_{1}=\left\{ j_{1}\right\} $ 

Then choose $j_{2}$ not in $C_{1},$and let $C_{2}=C_{j_{2}}$or
$\left\{ j_{2}\right\} $ depending on whether or not \{$j_{2}\}$
is a non return state. From the previous theorem, $C_{1}\cap C_{2}=\slashed{O}$
Continue the construction in this way to get $C_{1},C_{2},\ldots$

A non empty class, $C$, of state is such that to be closed if no
state outside $C$ is accessible from any state of $C$. 

i.e. $C$ is closed iff for every for every $\forall n>0\,\forall j\in C\,\forall k\notin C\;P_{jk}^{(n)}=0$, 

Examples: In the following examples we classify the states of the
corresponding Markov chains with the given transition matrices.
\begin{enumerate}
\item $\begin{bmatrix}1 & 0 & 0 & 0\\
\frac{1}{2} & \frac{1}{2} & 0 & 0\\
\frac{1}{3} & \frac{1}{3} & 0 & \frac{1}{3}\\
\frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4}
\end{bmatrix}$

\begin{itemize}
\item \{1\} closed communicating
\item \{2\} communication non closed
\item \{3-4\} communication non closed
\end{itemize}
\item $\begin{bmatrix}1 & 0 & 0 & 0\\
1 & 0 & 0 & 0\\
\frac{1}{2} & \frac{1}{2} & 0 & 0\\
\frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0
\end{bmatrix}$

\begin{itemize}
\item \{4\} non return 
\item \{1\} closed communicating
\item \{2\} non return (non closed)
\item \{3\} non return
\end{itemize}
\end{enumerate}
A \emph{closed communication} class $C$ of states is essentially
a Markov chains extracted and studied independently

If you write the transition matrix so that the states in $C$ are
written, then $P$ can be written in the form $P=\begin{bmatrix}P_{C} & 0\\
* & *
\end{bmatrix}$ where $*$ are possibly non-zero matrix and $P_{c}$ is the sub-matrix
whose entries are the transition probabilities between the states
of $C$. Then $P^{(n)}=P^{n}=\begin{bmatrix}P_{C}^{n} & 0\\
* & *
\end{bmatrix}$

Since a closed communication class $C$ can be extracted from a Markov
chain (by deleting all rows and colons corresponding to states outside
of $C$) and treated by itself as a Markov chains it follows that
for the study asymptotic properties of a Markov chain, it is sufficient
to restrict one self to chains in which there is exactly one closed
communicating class and all the other communicating class are non
closed


\subsection{Further Classification of states.}

In order to obtain certain asymptotic properties of a Markov chain,
we shall make further classification of states


\paragraph{Definition:}

For any state $j,k$ let $f_{jk}$ denote the probability starting
from $j$ the process will get into state $k$

Let $g_{kk}$ denote the probability that the chain will return infinitely
often to state $k$ given that it start in $k$


\paragraph{note:}

these definitions can be made mathematically rigorous by introducing
the r.v $N_{k}(n)$, the number of time that the state $k$ is visited
in $n$ step. Our descriptive definition will suffice


\paragraph{Definition:}

We denote $f_{jk}^{(n)}$ the conditional probability of entering
$k$ from $j$ from the first time in exactly $n$ steps 

the time it takes to enter $k$ from $j$ for the first time is called
the first passage time.


\paragraph{Note:}

$f_{jk}^{(n)}$ can be made precise by formally saying that if we
are to reach $k$ for the first time from $j$ in $n$ steps then
the chains must not have visited $k$ in the first $n-1$ steps and
then on the n\textsuperscript{th} step have jumped to state $k$


\paragraph{Example:}

Let $\left\{ X_{n},n=0,1,\ldots\right\} $ be a 2-states Markov chain
(with state 0 and 1) find the 1\textsuperscript{st}passage probs
$f_{jk}^{(n)}.$ Ex: $f_{00}^{(n)}=P_{01}P_{11}^{n-2}P_{10}\,n\geq2$


\paragraph{Proof:}

it's a mistake to say that the product (and power $n-2$) arise because
of independence of various event. They are certainly independent.
However we can condition backward and use the Markov property.

\begin{eqnarray*}
f_{00}^{(n)} & = & P\left[X_{1}=1,X_{2}=1,\ldots,X_{n-1}=1,X_{n}=0|X_{n}=0\right]\\
 & = & P\left[X_{n}=0|X_{0}=0,X_{1}=1,\ldots,X_{n-1}=1\right]P\left[X_{n-1}=1|X_{0}=0,\ldots;,X=1\right]\\
 &  & \cdots P\left[X_{1}=1|X_{0}=0\right]\\
 & = & P\left[X_{n}=0|X_{n-1}=1\right]P\left[X_{n-1}=1|X_{n-2}=1\right]\cdots P\left[X_{1}=1|X_{0}=0\right]\\
 &  & \mbox{[We used Markov property]}\\
 & = & P_{01}P_{11}^{n-2}P_{10}\\
 &  & \mbox{[We used Homogeneity}]
\end{eqnarray*}


The other $f_{jk}^{(n)}$'s follow similarly


\subsubsection*{Theorem:}

For any states $j$ and $k$ $f_{jk}=\sum_{n=1}^{\infty}f_{ik}^{(n)}$


\paragraph{Proof:}

Ever entering $k$ from $j$ is the disjoin union of entering $k$
from $j$ in exactly $n$ steps $\forall n\geq1$.


\subsubsection*{Theorem:}

For any states $j$ and $k$ and $n\geq1$ $P_{jk}^{(n)}=\sum_{m=1}^{n}f_{jk}^{(m)}P_{kk}^{m-n}$


\paragraph{Proof: }

If you are in state $k$ (starting in $j$) \emph{at} the n\textsuperscript{th}step
then you must have entered $k$ for the 1\textsuperscript{st}at some
$m$ and then in the remaining $n-m$ step, be in state $k$ (starting
from $k$)

we're talking of a disjoin union of these events. again we use the
Markov property and homogeneity to get the product


\subsubsection*{Theorem: }

For any state $j$ and $k$: 

\begin{eqnarray}
g_{kk} & = & \lim_{n\rightarrow\infty}\left(f_{kk}\right)^{n}\label{eq:A}\\
g_{jk} & = & f_{jk}g_{kk}\nonumber 
\end{eqnarray}



\paragraph{proof:}

$g_{kk}=\lim_{n\rightarrow\infty}P\left[\mbox{every returning to \ensuremath{k}from 1th \ensuremath{k}}\cap\mbox{ever returning from the second }k\cap\ldots\cap\mbox{ever returning from the nth}k\right]$

Now condition backward. 


\subsubsection*{Theorem}

For any state $k$ either $g_{kk}=0$ or $g_{kk}=1$, further 

$g_{kk}=0\Leftrightarrow f_{kk}<1$

$g_{kk}=1\Leftrightarrow f_{kk}=1$

Proof follows directly from \ref{eq:A}

After all this background we come to en important theorem: Theorem
{[}A zero-one|aw{]}


\section{Application to Branching process}

We show that in a simple (\noun{galton-watson}) Branching process,
the successive generation size either tend to infinity with probability
$1$ or die out with probability $1$.

Let $g_{kk}$ be the prob that the chain returns infinitely often
to the state $k$ we shall show that $g_{kk}=0\,\forall k=0,1,\ldots$
that is no matter what finite set of state you choose, you will visit
these states only a finite number of times.

To proves this all we need to do is show that $f_{kk}<1$

<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream=======<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
HEAD<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
HEAD=======>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
origin/master>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
Stashed changes<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream$g_{kk}=lim_{n\rightarrow\infty}(f_{kk})^{n}$ ======= <<<<<<< HEADg_{kk} = 


$g_{jk}=f_{jk}*g_{kk}$<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream 

To prove =======<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
HEAD to prove >\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
Stashed changes$g_{kk}=0$ for k=1,2, all we need to do is to show
that $f_{kk}<1,\,k=1,2,\ldots$

Now, $f_{kk}$= prob of ever return to k from k

The event denote A \{ever return to K from K\} $\subset$(denote B)
\{ not goint to 0 in the next generation from a generation of size
k\}

$P(A)\leq P(B)=1-(P_{0})^{k}$where $P_{0}$ = prob a parent wil have
children

$(P_{0})^{k}$ = prob that all parent will have 1 children (independant
of the number of children per parent)

If $P_{0}>0$, then $f_{kk}\leq1-(p_{0})^{k}<1$

$g_{kk}=0$ If $p_{0}>0,$ there is zero probability of making infinitely
many returns to any positive state. (generation size). That is, either
the generation sizes tend to infinity (may not monotonically) or a
generation w/ no individuals ($X_{n}=0)$ is reached, in which case,
of course, the state 0 will be visited thereafter. What about the
case $P_{0}=0$? The above argument gives $f_{kk}\leq1-0=1$ which
is not informative. However, this is the trivial case since each parent
has at least 1 child and the generation size must then increase to
infinity.

The following theorem help us decide whether $f_{kk}<1$ of $f_{kk}=1$

Theorem : For any state in a Markov chain, $f_{kk}<1$ <=> $\sum_{n=1}p_{kk}^{(n)}<\infty$, 

and $f_{kk}=1$ <=> $\sum_{n=1}^{\infty}$$p_{kk}^{(n)}$= +$\infty$
(will not be proven : uses much analysis)

proof sketch

Lemma: Let $p_{jk}(z)=\sum_{n=0}^{\infty}z^{n}p_{jk}^{(n)}=d_{jk}+\sum_{n=1}^{\infty}$z$^{n}p_{jk}^{(n)}$where
$d_{jk}=$1 if j=K and 0 otherwise

let $F_{jk}(z)$=$\sum_{n=1}^{\infty}z^{n}f_{jk}^{(n)}$ $P_{jk}(z)$
cd $F_{jk}(z)$ are defined for |z|<1 There are called generating
function

Proof |$P_{jk(z)}|$$\leq\sum_{n=0}^{\infty}|z|^{n}p_{jk}^{(n)}$$\leq$$\sum_{n=0}^{\infty}|z|^{(n)}$
======= 

To prove >\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
origin/master<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
HEAD$p_{jk}^{(n)}$ $\leq1$ which converges if |z|$<1$(gemometric
series)

Similirly for $F_{jk}(z)$

Theorem : For any two states j and is a MC and |z|<1

(1) $P_{kk}(z)=Fjk(z)P_{kk}(z)$ if j different k

(2) $P_{kk}$(z)=1=$F_{kk}(z)P_{kk}(z)$

(3) $P_{kk}$(z)= 1/1-$F_{kk}(z)$, $F_{kk}$(z)=1- 1/$P_{kk}(z)$ ======= -------------------------------------------------------------------------------- ------------------------------ ======= 
 for k=1,2, all we need to do is to show that $f_{kk}<1,\,k=1,2,\ldots$

Now, $f_{kk}$= prob of ever return to k from k

The event denote A \{ever return to K from K\} $\subset$(denote B)
\{ not goint to 0 in the next generation from a generation of size
k\}

$P(A)\leq P(B)=1-(P_{0})^{k}$where $P_{0}$ = prob a parent wil have
children

$(P_{0})^{k}$ = prob that all parent will have 1 children (independant
of the number of children per parent)>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
origin/master

If $P_{0}>0$, then $f_{kk}\leq1-(p_{0})^{k}<1$

$g_{kk}=0$ If $p_{0}>0,$ there is zero probability of making infinitely
many returns to any positive state. (generation size). That is, either
the generation sizes tend to infinity (may not monotonically) or a
generation w/ no individuals ($X_{n}=0)$ is reached, in which case,
of course, the state 0 will be visited thereafter. What about the
case $P_{0}=0$? The above argument gives $f_{kk}\leq1-0=1$ which
is not informative. However, this is the trivial case since each parent
has at least 1 child and the generation size must then increase to
infinity.

The following theorem help us decide whether $f_{kk}<1$ of $f_{kk}=1$


\section{Theorem 100}

\begin{eqnarray}
f_{kk}<1 & \Leftrightarrow & \sum_{n=1}^{\infty}P_{kk}^{(n)}<\infty\nonumber \\
f_{kk}=1 & \Leftrightarrow & \sum_{n=1}^{\infty}P_{kk}^{(n)}=+\infty\label{eq:th100}
\end{eqnarray}



\subsubsection*{proof:}

let's set $P_{jk}(z)=\sum_{n=0}^{\infty}z^{n}P_{jk}^{(n)}$ and $F_{jk}(z)=\sum_{n=0}^{\infty}z^{n}f_{jk}^{(n)}$

Claim: 
\begin{equation}
P_{kk}(z)=\frac{1}{1-F_{kk}(z)}\;,\,F_{kk}(z)=1-\frac{1}{P_{kk}(z)}\label{eq:71}
\end{equation}



\paragraph{Idea of how \ref{eq:71} is used to prove theorem 100 (\ref{eq:th100}).}

For the moment accept the following argument which is actually false.

using \ref{eq:71}, we have $F_{kk}(1)=1-\frac{1}{P_{kk}(1)}$. 

We note that $F_{jk}(1)=f_{kk}=\sum_{n=0}^{\infty}f_{jk}^{(n)}$ and
$P_{jk}(1)=\sum_{n=0}^{\infty}P_{jk}^{(n)}$.

Thus \ref{eq:71} says $f_{kk}=1-\frac{1}{\sum_{n=0}^{\infty}P_{kk}^{(n)}}$ 

We see immediatly that $f_{kk}<1\Leftrightarrow\sum_{n=1}^{\infty}P_{kk}^{(n)}<\infty$
and $f_{kk}=1\Leftrightarrow\sum_{n=1}^{\infty}P_{kk}^{(n)}=+\infty$

Now this argument is flowed since neither $P_{kk}(z)$ nor $F_{kk}(z)$
is defined for $z=1$. To get around this difficulty we need a result
in analysis that allows us to consider $\lim_{z\uparrow1}P_{kk}(1)$
and $\lim_{z\uparrow1}F_{kk}(1)$

There is an inportant limit theorem that will be used shortly:

For any state $k$, define $m_{kk}=\sum_{n=1}^{\infty}nf_{kk}^{(n)}$


\subsection{Rigorous proof}


\paragraph{Theorem: }

Let $k$ be any state such that $f_{kk}=1$ and $m_{kk}<\infty$.
We call $m_{kk}$ the expected (or mean) recurence (return) time to
state $k$. Then 
\begin{eqnarray}
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{m=1}^{n}P_{kk}^{(m)} & = & \frac{1}{m_{kk}}\label{eq:80}\\
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{m=1}^{n}P_{jk}^{(m)} & = & f_{jk}\frac{1}{m_{kk}}\label{eq:81}
\end{eqnarray}



\paragraph{Proof:}

A rigorous proof use generating function once again and a quite deep
theorem in analysis.

However we can give a plausible argument as to why (\ref{eq:80})
and (\ref{eq:81}) might be true.


\paragraph{Recall the (weak) law of large numbers:}

Let $X_{1},X_{2}$ be i.i.d r.v.s with $E(X)=\mu<\infty$ then $\frac{1}{n}\sum_{i=1}^{n}X_{i}\rightarrow\mu$
as $n\rightarrow\infty$


\paragraph{The strong law of large numbers:}

Under the same assumption $\frac{1}{n}\sum_{i=1}^{n}X_{i}(w)\rightarrow\mu$
as $n\rightarrow\infty$ with probability $1$ (almost surely)


\paragraph{Plausible argument}

Let the chain run until we made $n$ return to $k$ (from $k$) this
will require a random number of steps in the chain. Call this random
number $N_{kk}(n)$.

Let $T_{i}(k)$, for $i=1,2,\ldots$ be the observed return time to
state $k$ . The $T_{i}(k)$'s are easily seen to be i.i.d r.v.s Then
the observed proportion of time that the chain is in state $k$ is
given by $\frac{n}{\underset{N_{kk}(n)}{\underbrace{T_{1}(k)+\cdots+T_{n}(k)}}}$
we write this as $\frac{1}{\frac{1}{n}\sum_{i=1}^{n}T_{i}(k)}$.

Let $n\rightarrow\infty$ and by the Strong law of large number $\frac{1}{n}\sum_{i=1}^{n}T_{i}(k)\overset{\mbox{w.p 1}}{\rightarrow}m_{kk}$
as by definition <\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream$\left[E\left[T_{i}(k)\right]=m_{kk}\right]$ and
so 

How does this relate to (\ref{eq:80}) ?

We can evaluate the limiting proportion of time spent in state $k$
with a different scheme. Now allow the chain to run for $n$ transition

\begin{eqnarray*}
I_{m}(k) & = & 0\mbox{ if the chains is in state \ensuremath{k} at the \ensuremath{m^{th}} step}\\
 & = & 0\mbox{ otherwise}
\end{eqnarray*}


Then $\frac{1}{n}\sum_{m=1}^{n}I_{m}(k)$ is the observed proportion
of time that we are in state $k$ (out of $n$ steps)

We have $P\left[I_{m}(k)|X_{0}=k\right]=P_{kk}^{(m)}$ 

therefore $\frac{1}{n}\sum_{m=1}^{n}P_{kk}^{(m)}$is the expected
proportion of times in state $k$ (from $k$) in steps of the chain


\paragraph{Let's put this together}

We had $\frac{n}{\sum_{i=1}^{n}T_{i}(k)}\overset{\mbox{w.p 1}}{\rightarrow}\frac{1}{m_{kk}}$
and so by the dominate convergence theorem $E\left[\frac{n}{\sum_{i=1}^{n}T_{i}(k)}\right]\rightarrow E\left[\frac{1}{m_{kk}}\right]=\frac{1}{m_{kk}}$

i.e by the 1\textsuperscript{st} samplign scheme the expected proportion
of time in step $k\rightarrow\frac{1}{m_{kk}}$ 

$\frac{1}{n}\sum_{m=1}^{n}P_{kk}^{(m)}\rightarrow\frac{1}{m_{kk}}$


\section{Recurrent and Transient states and classes}


\paragraph{Definition:}

A state $k$ is said \emph{recurrent} if $f_{kk}=1$=======$\left[E\left[T_{i}(k)\right]=m_{kk}\right]$ >>>>>>> origin/master
 and so 

How does this relate to (\ref{eq:80}) ?

We can evaluate the limiting proportion of time spent in state $k$
with a different scheme. Now allow the chain to run for $n$ transition

\begin{eqnarray*}
I_{m}(k) & = & 0\mbox{ if the chains is in state \ensuremath{k} at the \ensuremath{m^{th}} step}\\
 & = & 0\mbox{ otherwise}
\end{eqnarray*}


Then $\frac{1}{n}\sum_{m=1}^{n}I_{m}(k)$ is the observed proportion
of time that we are in state $k$ (out of $n$ steps)

We have $P\left[I_{m}(k)|X_{0}=k\right]=P_{kk}^{(m)}$ 

therefore $\frac{1}{n}\sum_{m=1}^{n}P_{kk}^{(m)}$is the expected
proportion of times in state $k$ (from $k$) in steps of the chain


\paragraph{Let's put this together}

We had $\frac{n}{\sum_{i=1}^{n}T_{i}(k)}\overset{\mbox{w.p 1}}{\rightarrow}\frac{1}{m_{kk}}$
and so by the dominate convergence theorem $E\left[\frac{n}{\sum_{i=1}^{n}T_{i}(k)}\right]\rightarrow E\left[\frac{1}{m_{kk}}\right]=\frac{1}{m_{kk}}$

i.e by the 1\textsuperscript{st} samplign scheme the expected proportion
of time in step $k\rightarrow\frac{1}{m_{kk}}$ 

$\frac{1}{n}\sum_{m=1}^{n}P_{kk}^{(m)}\rightarrow\frac{1}{m_{kk}}$


\section{Recurrent and Transient states and classes}


\paragraph{Definition:}

A state $k$ is said \emph{recurrent} if $f_{kk}=1$>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
Stashed changes.


\paragraph{Definition:}

A state $k$ is said to be \emph{transient} if $f_{kk}<1$

Transient mean that $k$ will be visited only a finitely many time.

A class of state is said to be recurrent (transient) if all state
in it are recurrent (transient) 


\paragraph{Theorem: }

if $C$ is a communicating class of states in a Markov chain, then
if any state of $C$ is recurrent (transient) then all states in $C$
are recurrent (transient)


\subsubsection*{Example}

As an example consider the simple random walk on the integer. 

We shall show that the set of all state is recurent iff $p=\frac{1}{2}$
(an transient if not) 


\paragraph{Proof: }

first note that by the previous theorem is sufficient to show that
the state $0$ is recurrent iff $p=\frac{1}{2}$ . Reason: clearly
in the chain all states communicates.

We prove this claim by using Theorem 100 (\ref{eq:th100}) to show
that <\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream$\sum_{n=0}^{\infty}P_{00}^{(n)}$ ======= 
 iff $p=\frac{1}{2}$

We thus need an expression for <\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream$P_{00}^{(n)}$ ======= 
.

We first note that <\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<\textcompwordmark{}<
Updated upstream$P_{00}^{(2n-1)}=0\,n=1,2,\ldots$

Thus consider only the case $P_{00}^{(2n)}\,n=1,2,\ldots$, in this
case $P_{00}^{(2n)}=\binom{2n}{n}p^{n}\left(1-p\right)^{2n-n}\,n=1,2,\ldots$
because we need to do $n$ step to the left and then $n$ to the right 

so to examine $\sum_{n=0}^{\infty}P_{00}^{(n)}$ we need to examine
$\sum_{n=1}^{\infty}\binom{2n}{n}p^{n}\left(1-p\right)^{2n-n}=\sum_{n=1}^{\infty}\frac{2n!}{n!n!}p^{n}\left(1-p\right)^{2n-n}$

this looks bad because of factorials. Thus we use \noun{sterlings}
formula that gives an assymptotic expression fo $n!$ $n!\sim n^{n+\frac{1}{2}}e^{-n}\sqrt{2\pi}$
as $n\rightarrow\infty$. this mean $\lim_{n\rightarrow\infty}\frac{n!}{n^{n+\frac{1}{2}}e^{-n}\sqrt{2\pi}}=1$
hus if we use this formula over our sum we get $P_{00}^{(2n)}\sim\frac{\left[4p\left(1-p\right)\right]^{n}}{\sqrt{\pi n}}$
finally examinating $\sum_{n=0}^{\infty}P_{00}^{(n)}$ reduce to examinating
$\sum_{n=1}^{\infty}\frac{\left[4p\left(1-p\right)\right]^{n}}{\sqrt{\pi n}}$ 

this the power series $\sum_{n=1}^{\infty}a_{n}x^{n}$ with $x=4p(1-p)$
and $a_{n}=\frac{1}{\sqrt{\pi n}}$ converge if and only if $p\neq\frac{1}{2}$
and diverge if $p=\frac{1}{2n}$ so by theorem 100 (\ref{eq:th100}),
the state is recurent if and only if $p=\frac{1}{2}$ (and transient
if $p\neq\frac{1}{2}$ )


\paragraph{Notes:}
\begin{enumerate}
\item Intuitively if $p\neq\frac{1}{2}$ there's a drift in one way away
from $0$ avoiding recurence
\item since all state clearly communicate \emph{all} state are recurrent
if and only if $p=\frac{1}{2}$ and transient $p\neq\frac{1}{2}$
\item It can be shown that in 2 dimension the simple random walk is recurent
iff $p=\frac{1}{4}$
\item In 3 or more dimension the simple random walk is \emph{always} transicient,
even if probability of moving in any direction is the same
\end{enumerate}

\subsubsection*{Theorem : }

A recurrent communicating class is closed. A (transient ???) closed
communicating class must possess infinitely manu states.


\paragraph{Proof: }

We show that $f_{kk}=1$ and k->j implies $f_{jk}=1$ i.e. $k\leftrightarrow j$
{[}i.e. the only way that you can get to a state j is if that if that
state is in $C.$This implies $C$ be closed.


\paragraph{Proof: }

Intuitively does this seem right. Well, we know that there is a path
from k to j with positive prob. This cannot be a ``bad'' path, in
the sense that with w.p. we can get back to k from j, since we would
not be able to say w.p. 1 we can g from k to k. Thus we must have
that $j\rightarrow k$ i.e. $k\leftrightarrow j$.


\paragraph{Formalities: }

For any state k and integer n , $g_{kk}=\sum_{allstates}p_{ki}^{(n)}g_{ik}$
(obvious + you have to be somewhere after n steps (starting in k)
and then make infinitely many returns to k from that ``somewhere'')

$1-g_{kk}=\sum_{allstates}p_{ki}^{(n)}(1-g_{ik})$since $g_{kk}=1,$
$1-g_{kk}=0$ and hence, $\sum_{states}p_{ki}^{(n)}\{1-g_{ik}\}$
for every n

Now return to thr state $j$ that we were considering since $k\rightarrow j$
$\exists$ an N such that $p(N\text{)}_{kj}>0.$It follows that $(1-g_{jk})=0$
i.e. $g_{jk}=1$. Now, $g_{jk}=$$f_{jk}g_{kk}$means that if k is
reccurent and hence $g_{kk}=1$ we mush have $f_{jk}=1,$ which is
what we wanyed to show. To prove that a transient closed communicating
class must be infinite, we note that for any state j, $g_{jk}=0$
if k is transient. 

I.e. consequently, if $C$ is a closed and transient communicating
class then C me possess infinitely many states.

Reason: With prob 1 the chain remains only for a finite number of
steps in any finite set of transiant states. Done! We can now strengthen
our theorem on the partition of the states of a chain into disjoint
communicating classes and non-return states.


\subsubsection*{Theorem}

The set S, of return states of a Markov chain, can be written as the
union of disjoint communcating classes, 

S = $C_{1\cup}C_{2}...where$ class $C,$ is either (i) closed and
recurrent, (ii) closed and transient or (iii) non closed and transiant. 

CLosed Non closed

recurrent `/fine/ does not exist

transiant/ does not exist in a finite chain / fine


\subsubsection*{New Terminology}
\begin{enumerate}
\item if all state of a Markov chain communicate we say that this chain
is \emph{irreductible}
\item In a markov chain if $f_{nn}=1$ i.e the chain is recurrent, we say
that the state $k$ is \emph{positive recurrent} if $m_{kk}<0$ (i.e
the exepected time to return to $k$ from $k$ is finite)


If $m_{kk}=+\infty$ the we say the state is null recurrent

\item Returning to the randoim walk it turns out that even in $1$ dimension
and $p=\frac{1}{2}$ , $m_{kk}=+\infty$\\
i.e the simple random walk is null recurrent=======$P_{kk}^{(2n-1)}=0\,n=1,2,\ldots$
\end{enumerate}
Theorem : A recurrent communicating class is closed. A closed communicating
class must possess infinitely manu states.

Proof: We show that $f_{kk}=1$ and k->j implies $f_{jk}=1$ i.e.
k<->j {[}i.e. the only way that you can get to a state j is if that
if that state is in $C.$This implies $C$be closed.

Proof: Intuitively does this seem right. Well, we know that there
is a path from k to j with positive prob. This cannot be a ``bad''
path, in the sense that with w.p. we can get back to k from j, since
we would not be able to say w.p. 1 we can g from k to k. Thus we must
have that j -> k i.e. k<->j.

Formalities: For any state k and integer n , $g_{kk}=\sum_{allstates}p_{ki}^{(n)}g_{ik}$
(obvious + you have to be somewhere after n steps (starting in k)
and then make infinitely many returns to k from that ``somewhere'')

1-$g_{kk}$ = $\sum_{allstates}p_{ki}^{(n)}(1-\mbox{g\_ik})$ since
$g_{kk}=1,$ 1-$g_{kk}=0$ and hence, $\sum_{states}p_{ki}^{(n)}\{1-g_{ik}\}$
for every n

Now return to thr state j that we were considering since k->j, $\exists$
an N such that $p(N\text{)}_{kj}>0.$It follows that (1-$g_{jk})=0$
i.e. $g_{jk}=1$. Now, $g_{jk}=$$f_{jk}g_{kk}$means that if k is
reccurent and hence $g_{kk}=1$ we mush have $f_{jk}=1,$ which is
what we wanyed to show. To prove that a transient closed communicating
class must be infinite, we note that for any state j, $g_{jk}=0$
if k is transient. 

I.e. consequently, if $C$is a closed and transient communicating
class then C me possess infinitely many states.

Reason: With prob 1 the chain remains only for a finite number of
steps in any finite set of transiant states. Done! We can now strengthen
our theorem on the partition of the states of a chain into disjoint
communicating classes and non-return states.

Theorem: The set S, of return states of a Markov chain, can be written
as the union of disjoint communcating classes, 

S = $C_{1\cup}C_{2}...where$ class $C,$ is either (i) closed and
recurrent, (ii) closed and transient or (iii) non closed and transiant. 

CLosed Non closed

recurrent `/fine/ does not exist

transiant/ does not exist in a finite chain / fine>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>\textcompwordmark{}>
Stashed changes
\end{document}
